[{"authors":["admin"],"categories":null,"content":"Yuan Du is an experienced Statistician at AdventHealth in Orlando, FL with years of professional experience in the academic and Healthcare research setting. Her research experience includes Medical research, Healthcare policy, Clinical trials, etc. She is also working on her PhD in Big Data Analytics. Her PhD reserach is on Energy Based Models, Generative Models, Computer Vision. This is her blog that contains topics in Statistics, Data Science and Deep Learning.\nYou may find Yuan\u0026rsquo;s last name as Du or Eldaif since she married Bassem Eldaif, MD in 2019. If you want to find out more about Urologic Robotic Surgery, please check out his practice website Center for Advanced Urology \u0026amp; Robotics.\n","date":1586131200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1586131200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://yuan-du.com/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Yuan Du is an experienced Statistician at AdventHealth in Orlando, FL with years of professional experience in the academic and Healthcare research setting. Her research experience includes Medical research, Healthcare policy, Clinical trials, etc. She is also working on her PhD in Big Data Analytics. Her PhD reserach is on Energy Based Models, Generative Models, Computer Vision. This is her blog that contains topics in Statistics, Data Science and Deep Learning.","tags":null,"title":"Yuan Du","type":"authors"},{"authors":[],"categories":["Statistics","Machine Learning","Data Science","Deep Learning"],"content":"\rPreviously, in the post Loss Functions in Machine Learning and LTR we disscussed about how loss functions were used in ML and briefly mentioned LTR. Here I’ll discuss about LTR. LTR uses Machine Learning (ML)/Artifical Intelligence (AI) to predict rankings/ordinal data. It’s useful for google search, drug discovery, bioinformatics. Here is a list that seperates traditional ML from LTR:\n\rSolve a ranking on a list of items\n\rPredict the optimal ordering of the list\n\rDoesn’t care much about the score of each item/point\n\ronly care the relative score/ordering among all the items\n\r\rFor example, if we have 2 ML models to predict students’ score. and our goal is to rank students. and we have below results from the ML models. In this case, Model 2 is better at ranking compared to Model 1 even though Model 1 has better prediction accuracy. Rank error is pair-wise based and is defined as \\(\\frac{ \\# \\textrm{ of discordant pairs} }{ \\#\\textrm{ of total pairs between + and -} }\\).\n\r\rStudent\rTrue Score\rModel 1\rModel 2\r\r\r\rStudent1\r90%\r88%\r100%\r\rStudent2\r85%\r89%\r50%\r\rStudent3\r80%\r83%\r10%\r\r\r\rLTR system includes bipartite ranking, k-partite ranking, real value based ranking. We only talk about bipartite ranking here.\n1. Bipartite RankSVM Algorithm\nBipartite RankSVM Algorithm uses hinge loss. The hinge loss is a loss function used for “maximum-margin” classification, most notably for support vector machine (SVM). It’s equivalent to minimize the loss function \\(L_{hinge}(f,x_i^+,x_i^-) = [1-(f(x_i^+)-f(x_i^-))]_+ [u_+ = max(u,0)]\\)\nWith \\(f = W * X =\\) ranking score, the optimization problem is loss + penalty:\r\\[ \\min_{f \\in F_k} \\frac{1}{mn}\\sum_{i=1}^{m} \\sum_{j=1}^{n}L_{hinge}(f,x_i^+,x_i^-) + \\frac{\\lambda}{2}||f||_k^2 \\]\nThus, the term \\(f(x_i^+)-f(x_i^-)\\) the larger, the better.If \\(f(x_i^+)-f(x_i^-) \u0026lt;0\\), it means that it’s making mistakes so the objection function is penalized.\n2. Bipartite RankBoost Algorithm\nBipartite RankBoost Algorithm uses the exponential loss.\nThe population minimizer is:\r\\[\\min_{f \\in L(F_{base})} \\frac{1}{mn}\\sum_{i=1}^{m} \\sum_{j=1}^{n}L_{exp}(f,x_i^+,x_i^-)\\]\nwhere \\(L_{exp}(f,x_i^+,x_i^-) = exp(-f(x_i^+)-f(x_i^-))\\).\n3. Bipartite RankNet Algorithm\nBipartite RankNet Algorithm uses the logistic loss (binomial log-likelihood loss or cross entropy loss).\nThe binomial log-likelihood loss function is:\r\\[\\min_{f \\in F_{neural}} \\frac{1}{mn}\\sum_{i=1}^{m} \\sum_{j=1}^{n}L_{logistic}(f,x_i^+,x_i^-)\\]\nwhere \\(L_{logistic}(f,x_i^+,x_i^-) = log(1+ exp((-f(x_i^+)-f(x_i^-)))\\).\n\nReference:\nComputer Science \u0026amp; Artificial Intelligence Laboratory, MIT\n","date":1623024000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623119249,"objectID":"0f602fb842137eff536b66cabfb01031","permalink":"https://yuan-du.com/post/2021-06-07-ltr/decision-theory/","publishdate":"2021-06-07T00:00:00Z","relpermalink":"/post/2021-06-07-ltr/decision-theory/","section":"post","summary":"Previously, in the post Loss Functions in Machine Learning and LTR we disscussed about how loss functions were used in ML and briefly mentioned LTR. Here I’ll discuss about LTR. LTR uses Machine Learning (ML)/Artifical Intelligence (AI) to predict rankings/ordinal data. It’s useful for google search, drug discovery, bioinformatics. Here is a list that seperates traditional ML from LTR:\n\rSolve a ranking on a list of items\n\rPredict the optimal ordering of the list","tags":["Machine Learning","Data Science","Decision Theory","LTR","Deep Learning"],"title":"Learning To Rank (LTR)","type":"post"},{"authors":["James Yu","Jingxin Sun","Yuan Du","Rushang Patel","Juan Carlos Varela","Shahram Mori","Chung-Che Chang"],"categories":null,"content":"","date":1622246400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622246400,"objectID":"353812a0ebc84ce4ae3d155745d244ee","permalink":"https://yuan-du.com/publication/2021-5-diag/","publishdate":"2021-05-29T00:00:00Z","relpermalink":"/publication/2021-5-diag/","section":"publication","summary":"","tags":["AML","Gene mutation","Survival","DNA methylation","Precision medicine","Prognosis"],"title":"Adverse Impact of Mutations in DNA Methylation Regulatory Genes on the Prognosis of AML Patients in the 2017 ELN Favorable Risk Group","type":"publication"},{"authors":["James Yu","Yuan Du","Sarfraz Ahmad","Rushang D Patel","Juan Carlos Varela","Chung-Che Chang","Shahram Mori"],"categories":null,"content":"","date":1618963200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618963200,"objectID":"3d0cd944271d83872837b1f40b99f3d8","permalink":"https://yuan-du.com/publication/2021-4-tct/","publishdate":"2021-04-21T00:00:00Z","relpermalink":"/publication/2021-4-tct/","section":"publication","summary":"","tags":["AML;","Allogeneic","Flow cytometry","Minimal residual disease","Myeloablative","Reduced-intensity conditioning","Stem cell","Survival","Precision medicine","Prognosis"],"title":"Comparison of Myeloablative versus Reduced-Intensity Conditioning Regimens in Allogeneic Stem Cell Transplantation Recipients with Acute Myelogenous Leukemia with Measurable Residual Disease-Negative Disease at the Time of Transplantation: A Retrospective Cohort Study","type":"publication"},{"authors":["Saeed Ali","Neelam Khetpal","Evgeny Idrisov","Asad Ur Rahman","Sameen Khalid","Yuan Du","Udayakumar Navaneethan","Shyam Varadarajulu","Robert Hawes","Muhammad Khalid Hasan"],"categories":null,"content":"","date":1617235200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617235200,"objectID":"44abb393d32f2bc3e4853a02e00675ce","permalink":"https://yuan-du.com/publication/2021-4-smj/","publishdate":"2021-04-01T00:00:00Z","relpermalink":"/publication/2021-4-smj/","section":"publication","summary":"At SC1, early recurrence was noted in 77 of 354 (21.8%) lesions; 76 (98.7%) were treated endoscopically. The remaining 277 of 354 (78.2%) lesions had no recurrence at SC1; only 41 lesions (15%) were followed up at SC2. Recurrence at SC2 was found in 4 lesions (9.8%), all of which were treated endoscopically. Lesion size 40 mm was associated with recurrence. Recurrence at both SC1 and SC2 was successfully treated endoscopically in 78 of 81 lesions (96.3%).","tags":["EMR","SMI","Logistic regression"],"title":"Colon mucosal neoplasia referred for endoscopic mucosal resection: Recurrence of adenomas and prediction of submucosal invasion","type":"publication"},{"authors":[],"categories":["Statistics","Machine Learning","Data Science","Deep Learning"],"content":"\r\rPreviously, decision theory was disscussed and an important part of is to evaluate a decision rule for decision making. Since the risk is the average loss \\(L(\\theta,d)\\), different loss functions were used in Machine Learning models. Mean squared error (MSE) was mentioned as the most famous meausre by using squared error loss proposed by Gauss. Regression, Linear discriminant analysis (LDA) use squared error loss. The squared loss function tends to penalize outliers excessively, leading to slower convergence rates. There are other popular loss functions and they are applied in various Machine Learning and Deep Learning models. The plot shows the loss function for two class classification.\nLoss Functions: \"Loss vs y*f(x); \\(y= \\pm 1\\), the prediction is f, with class prediction sign(f)\"\n\r1. Hinge loss\nThe hinge loss is a loss function used for “maximum-margin” classification, most notably for support vector machine (SVM).It’s equivalent to minimize the loss function \\(L(y,f) = [1-yf]_+\\).\nWith \\(f(x) = h(x)^T \\beta + \\beta_0\\), the optimization problem is loss + penalty:\r\\[ \\min_{\\beta_0,\\beta} \\sum_{n=1}^{\\infty}[1-y_if(x_i)]_+ + \\frac{\\lambda}{2}||\\beta||^2 \\]\n2. Exponential loss\nThe exponential loss is convex and grows exponentially for negative values which makes it more sensitive to outliers. The exponential loss is used in the AdaBoost algorithm. The principal attraction of exponential loss in the context of additive modeling is computational. The additive expansion produced by AdaBoost is estimating onehalf of the log-odds of P(Y = 1|x). This justifies using its sign as the classification rule.\nThe population minimizer is:\r\\[f^*(x) = \\arg\\min_{f(x)} E_{Y|x}(e^{-Yf(x)}) = \\frac{1}{2} log\\frac{Pr(Y = 1|x)}{Pr(Y = -1|x)}\\]\ror\r\\[Pr(Y = 1|x) = \\frac{1}{1+e^{-2f*(x)}}\\]\r3. Logistic loss(Binomial Deviance)\nThe logistic loss is also called as binomial log-likelihood loss or cross entropy loss. It’s used for logistic regression and in the LogitBoost algorithm. The cross entropy loss is ubiquitous in deep neural networks/Deep Learning.\nThe binomial log-likelihood loss function is:\r\\[l(Y,p(x)) = Y\u0026#39;logp(x) + (1-Y\u0026#39;)log(1-p(x))\\]\nor the deviance\r\\[-l(Y,f(x) = log(1+e^{-2Yf(x)})\\]\nSummary Table\n\r\r\r\rLoss Function\r\\(L[y,f(x)]\\)\rMinimizing Function\rAdvantage/Disadvantage\r\r\r\rSquared loss\r\\([y-f(x)] = [1-yf(x)]^2\\)\r\\(2Pr(Y=+1|x)-1\\)\reasy cross validation of regularization parameters/slower convergence rates\r\rHinge loss\r\\([1-yf]_+\\)\r\\(sign[Pr(Y=+1|x)-\\frac{1}{2}]\\)\rsupport points/not differentiable at\\(yf(x)=1\\)\r\rExponential loss\r\\(\\frac {1}{2} log(1+e^{-Yf(x)})\\)\r\\(\\frac{1}{2}log\\frac{Pr(Y =+1|x)}{Pr(Y =-1|x)}\\)\rgrows exponentially for negative values,more sensitive to outliers\r\rLogistic loss\r\\(log(1+e^{-Yf(x)})\\)\r\\(log\\frac{Pr(Y =+1|x)}{Pr(Y =-1|x)}\\)\rgrows linearly for negative values,less sensitive to outliers\r\r\r\rSimilarities between loss functions: \n\rHinge loss, Exponential loss, Logistic loss have very similar tails, giving zero penalty to points well inside their margin and linear or exponential penalty to points on the wrong side adn far away. Squared error loss gives a quadratic penalty and points inside their own margin have a strong influence on othe model as well.\n\rExponential loss and Logistic loss have the same asymptotes as the SVM hinge loss but are rounded in the interrior.\n\r\r\nThe above popular loss functions are also used in deep learning, for example, Learing To Rank (LTR) for a Recommender System (RS). Differently from traditional machine learning problem that’s to predict the target either classification or regression, LTR optimizes the ranking accruacy instead of the prediction probability accuracy.\nRanking is useful in our daily life for Recommendation system like Netflix, Amazon; Information Retrieval like goole; Drug discovery; Bioinformatics. Generally speaking, there are three types of rankings: bipartie ranking, k-partite ranking, real-valued labels based ranking. RankSVM, RankBoost, RankNet with corresponding loss functions are used for the ranking problems. A seperate post will be written to further demonstrate LTR framework and how the loss functions are used.\n\nReference:\nHastie, T., Tibshirani, R., \u0026amp; Friedman, J. H. (2009). The elements of statistical learning: data mining, inference, and prediction. 2nd ed. New York: Springer.\rComputer Science \u0026amp; Artificial Intelligence Laboratory, MIT\n","date":1607912849,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607912849,"objectID":"e8292f46f663c85ac73b2083d581a5af","permalink":"https://yuan-du.com/post/2020-12-13-loss-functions/decision-theory/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/2020-12-13-loss-functions/decision-theory/","section":"post","summary":"Previously, decision theory was disscussed and an important part of is to evaluate a decision rule for decision making. Since the risk is the average loss \\(L(\\theta,d)\\), different loss functions were used in Machine Learning models. Mean squared error (MSE) was mentioned as the most famous meausre by using squared error loss proposed by Gauss. Regression, Linear discriminant analysis (LDA) use squared error loss. The squared loss function tends to penalize outliers excessively, leading to slower convergence rates.","tags":["Machine Learning","Data Science","Decision Theory","LTR","Deep Learning"],"title":"Loss Functions in Machine Learning and LTR","type":"post"},{"authors":["Jessica Andrews","Patricia Louzon","Xavier Torres","Eric Pyles","Mahmood H. Ali","Yuan Du","John W. Devlin"],"categories":null,"content":"","date":1604880000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604880000,"objectID":"b115bd4dbd56fa2f8e1ed049c2298ceb","permalink":"https://yuan-du.com/publication/2020-11-aop/","publishdate":"2020-11-08T00:00:00Z","relpermalink":"/publication/2020-11-aop/","section":"publication","summary":"Groups before (n = 48) and after (n = 29) sleep protocol implementation were well matched. After protocol implementation, patients had a longer TST (389 ± 123 vs 310 ± 147 minutes; P = 0.02) and better RCSQ-perceived sleep quality (63 ± 18 vs 42 ± 24 mm; P = 0.0003) compared with before implementation.","tags":["Fitbit","sleep quality","ICU","Pharmacy","Repeated Measures","Mixed model"],"title":"Impact of a Pharmacist-Led Intensive Care Unit Sleep Improvement Protocol on Sleep Duration and Quality","type":"publication"},{"authors":["James Yu","Jingxin Sun","Yuan Du","Chung-Che Chang"],"categories":null,"content":"","date":1604534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604534400,"objectID":"e59a265feed1c6bd6849518aa8a56638","permalink":"https://yuan-du.com/publication/2020-11-blood/","publishdate":"2020-11-05T00:00:00Z","relpermalink":"/publication/2020-11-blood/","section":"publication","summary":"In 2017, the European LeukemiaNet (ELN) stratified AML patients into 3 risk groups based on the presence or absence of specific chromosome abnormalities and selected gene mutations.1 However, some studies have suggested that the prognosis or CR rates are heterozygous in the 2017 ELN favorable group depending on the specific co-existing mutations.2 Although they were not included in 2017 ELN due to an insufficient accumulation of evidence, some mutations involved in DNA methylation, including DNMT3A and IDH1/2, revealed adverse effects in AML prognosis.1,3-5 The goal of the current study was to evaluate how the DNA methylation regulatory (DNA-MR) gene mutations may impact the outcomes of AML patients categorized as favorable risk according to the ELN risk stratification.","tags":["AML","Gene mutation","Survival","DNA methylation","Precision medicine","Prognosis"],"title":"Adverse Impact of Mutations in DNA Methylation Regulatory Genes on the Prognosis of AML Patients in the 2017 ELN Favorable Risk Group","type":"publication"},{"authors":["James Yu","Jingxin Sun","Yuan Du","Chung-Che Chang"],"categories":null,"content":"","date":1604534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604534400,"objectID":"80fcc92273b70e074706a6bf10aea241","permalink":"https://yuan-du.com/publication/2020-11-blood2/","publishdate":"2020-11-05T00:00:00Z","relpermalink":"/publication/2020-11-blood2/","section":"publication","summary":"Recently, genomic mutation profiling of leukemic cells has been actively studied and some results have been integrated into the 2017 ELN classification with cytogenetic analysis for risk assessment of AML populations.1 However, only a few mutations are well identified and included in the 2017 ELN classification. In addition, except interaction between NPM1 and FLT-ITD, correlation and co-occurrence among various mutations have not been well studied. Here we describe our single center genomic landscapes of 2017 ELN guideline components with other NGS mutations in adult AML.","tags":["AML","Genomic Landscape","Survival","Precision medicine"],"title":"Genomic Landscape of Acute Myeloid Leukemia (AML) on the Basis of 2017 ELN Classification and Other Mutations in Adult AML - Single Healthcare System Data","type":"publication"},{"authors":[],"categories":["Statistics","Machine Learning","Data Science"],"content":"\rStatistics starts with probability theory, particularly in the analysis of games of chance. To be refferred to as a game, it involves three elements mathmatically:\nParameter space \\(\\Theta\\), a population characteristics, a physical quantity for example, mean.\n\rActions/Decisions space \\(\\mathscr{D}\\) available to statistician.\n\rA loss function, \\(L(\\theta,d)\\), a real-valued function defined on \\(\\Theta \\times \\mathscr{D}\\).\n\r\rThus, any such triplet \\((\\Theta, \\mathscr{D}, L )\\) defines a game. For example, Black jack, poker, chess, tic-tac-toe and so on are games that are played by strategy. “Game Theory” was proposed by two economists: John Von Neuman and John Nash in 1950s. Two or more players competing against one another. Neither player generally knows the others’ strategy. The goal of the game is to pick a strategy that guarentees he/she can’t be “too bad”.\nReal life examples: \nProduct pricing decisions:  Seasonal promotions allow retailers to sell more stock of products and consumers to get best deals. The focus of retailers is on using the best pricing strategy while the preference of consumers is to choose the best deal in terms of discount and variety.\nInvestment decisions:  The different distributions of the investment on bond, stocks, short-term reserves will result in different returns. A historical risk/return (1926-2018) can be found at Vanguard portfolio allocation models.\nPrisoners’ dilemma: The moral of the story in terms of decisions in a legal setting: You have the right to stay silent and please shut the f* up and let your attoney to do the talk.\nMore examples can be found in this post.\nDecision theory is similar to the game theory. The main differences are :\nIn the statistical context, the players are the statistician and “Nature”, who knows the true value of the parameter. In two-player game, both are trying simultaneously to maximize their winnings, whereas in decision theory nature chooses a state without this view in mind.\n\rAll statistical games allow statistician to gain information by sampling. However, it is the exploitation of the structure which such gathering of information gives to a game that distinguishes decision theory from game theory proper.\n\r\rA real life example:\nMedical diagnosis: Sometimes you never know until you open up the patient to see if the cancer is absent because of the limitations on imaging diagnosis. A surgeon needs to decide if a surgery (an action/ a decision) is necessary based on if the patient has cancer or not. There are 4 combinations between the 2 decisions and 2 conditions, thus 4 outcomes scored by %.\nCombination 1: The presence of cancer is confirmed and the surgeon decides to perform a surgery. The score is 100% because that’s the best decision.\nCombination 2: There is presence of cancer and the surgoen decides not to perform a surgery. The score is 0% because that’s the worst consequence.\nCombination 3: Cancer is absent and the surgeon decides to perform a surgery. The score is 40% because it doesn’t results in serious consequence.\nCombination 4: Cancer is absent and surgoen decides not to perform a surgery. The score is 85% because it’s a good decision and no consequence as well.\n\nModels, Decision rules and Risk\nStatistical model (class or family of distributions): The parameter \\(\\theta\\) and data \\(X\\) are related through a model in which the distribution of \\(X\\) is determined by \\(\\theta\\) The distribution when the parameter \\(\\theta\\) is denoted \\(P_\\theta\\) and we write \\(X \\sim P_\\theta\\). Formally, a model is written as the set of distributions for \\(X\\), \\(\\mathscr{P} = {P_\\theta: \\theta \\in \\Theta}\\).\nDecision rules: a non-randomized decision rule is a function \\(\\delta : \\mathscr{X} -\u0026gt; \\mathscr{D}\\)\n\rThe set of decision rules: \\(\\mathscr{D} =\u0026gt; d \\in \\mathscr{D}, \\theta \\in \\mathscr{D}\\) and \\(\\delta(x) \\in \\mathscr{D}\\) \\(d=\\delta(x)\\)\n\r\\(\\delta\\) prescribes a course of action for every observable dataset \\(x\\in \\mathscr{X}\\)\n\r\rRisk: to evaluate a decision rule, we use risk \\(R(\\theta, \\delta)\\). It is the average loss \\(L(\\theta,d)\\) between the estimand \\(\\theta\\) and the estimator \\(d\\) –\u0026gt; an expected loss \\(E_\\theta\\{ L[\\theta,\\delta(X)] \\}\\). One wants the estmator \\(\\delta(x)\\) to be accurate, but just what measure of accuracy should be used is fairly arbitrary. Mean squared error (MSE) is the most famous measure. In 1820s, Gauss proposed the square of the error as a measure of loss. He defends his choice by an appeal to mathmematical simplicity and convenience.\n\nReference:\n1. Mathematical Statistics: A Decision Theoretic Approach by Thomas S. Ferguson, Academic Press; 1st edition\n2. Theoretical Statistics: Topics for a Core Course by Robert W. Keener, Springer; 2010 edition\n3. Theory of Point Estimation by Erich L. Lehmann, George Casella, Springer; 2nd edition\n","date":1600819200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600914449,"objectID":"fb46ee578e86b585bb766b3f6fbfbc5d","permalink":"https://yuan-du.com/post/2020-09-23-decision-theory/decision-theory/","publishdate":"2020-09-23T00:00:00Z","relpermalink":"/post/2020-09-23-decision-theory/decision-theory/","section":"post","summary":"Statistics starts with probability theory, particularly in the analysis of games of chance. To be refferred to as a game, it involves three elements mathmatically:\nParameter space \\(\\Theta\\), a population characteristics, a physical quantity for example, mean.\n\rActions/Decisions space \\(\\mathscr{D}\\) available to statistician.\n\rA loss function, \\(L(\\theta,d)\\), a real-valued function defined on \\(\\Theta \\times \\mathscr{D}\\).\n\r\rThus, any such triplet \\((\\Theta, \\mathscr{D}, L )\\) defines a game.","tags":["Statistics","Data Science","Decision Theory","Game Theory"],"title":"Game Theory and Decision Theory","type":"post"},{"authors":null,"categories":null,"content":"Read Report \nA sample code \nSummary: This stack modeling approach is quite unique based on the data distribution. Choosing the appropreciate machine learning method based on the data distribution is the key of the success.\nIt would be interesting to see similar problems in a global multi-locations problem, where each location has multivariate normal distribution but globally the data is non-normal with high kutotusis.\n","date":1597622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597622400,"objectID":"6d9d72177885f85dd382fa44648c39d1","permalink":"https://yuan-du.com/project/kaggle-project/","publishdate":"2020-08-17T00:00:00Z","relpermalink":"/project/kaggle-project/","section":"project","summary":"Target (binary) Classification predication.","tags":["Machine Learning"],"title":"Kaggle Instant Gratification Competition Project","type":"project"},{"authors":[],"categories":["Statistics","Machine Learning","Data Science"],"content":"\r\rThere are so many terms regarding the field of Statistics and Data Science. We often heard Statistics, Data Mining, Machine Learning, Big Data, etc. It especially confuses people that’s in a different field. I remember that over five years ago, a radiologist asked me if I can mine data from the radiology system because she saw that I have Data Mining skills. I was blown away by the understanding of Data Mining to a doctor. Data Mining and data extraction is totally different. After data extraction and data preparation, data mining is used to identify patterns and relationships based on the research/business questions.\nGenerally speaking, due to the storage and advancement of computers, our data analysis power which builds on Statistical knowledge expanded by using more complicated statistical theory and algorithms that are applied to multidisciplinary science such as Biostatistics, Medicine, Public Health, Computer Science, Engineering, Physicis, etc.\rNature has a paper “Statistics versus machine learning” that explains the relationships.From Data Mining to Knowledge Discovery in Databases discussed and summrized the history of Knowledge discovery of database (KDD).\nIn the realm of healthcare research studies, I would like to share my own experience of what types of statistical learning were used. Based on the objectives of a study, we generally have two types of goals:\n\rInference: Identify risk factors that associate with response outcome(s). It normally has smaller sample size. This is the most common goal in medical research. It requires clinical knowledge to start with research questions that involve hypothesis. Univariate analysis (Hypothesis testing) and Multivariable analysis are used. Both types of analysis need assumptions on the data distribution, variance and linear/nonlinear relationship with response variable(s) to perform correct statistical tests. For univariate analysis, please check out my slides for the most commonly used hypothesis testings. The most common problem is significance (p-value) fishing. There are difference p-value adjustment methods to consider when there are multiple testings. Physicians/researchers often want to publish significant testing result only which is not healthy for medical research. Non significant factors are important to the literature. It’s useful for meta analysis. For multivariable analysis, here are some examples that difference statistical models were used:\n\rClinical Outcome Study: Comparison of hospital outcomes and resource utilization in acute COPD exacerbation patients managed by teaching versus non-teaching services in a community hospital: The data was from both national database Premier and hospital EHR(Electric Health Record). Multiple logistic regression was used for the multivariable analysis to identify the factors that contribute to resource utilization in the acute COPD patients.\n\rCommercial Device Study: Characterisation of ICU sleep by a commercially available activity tracker and its agreement with patient-perceived sleep quality: This data was collected from ICU patients that used fitbits as alternative sleep tracking devices. Since each patient was measured several times, a mixed model repeated measure was used to detect the correlation/agreement between each sleep quality measure and the gold standard Richard-Campbell Sleep Questionnaire (RCSQ). Instead of a single pearson correlation coefficient, the bootstrap method with 1000 times was implemented to generate Confidence Interval for statistical inference.\n\rPopulation Health Study: Contextual, Organizational and Ecological Effects on the Variations in Hospital Readmissions of Rural Medicare Beneficiaries in Eight Southeastern States: This a longitudinal study funded by NIH. Data was from mainly from CMS data warehouse on beneficiaries and providers. First, risk-adjusted readmission was calculated by Logistic regression model on patient level. Then Generalized Estimating Equation (GEE) method was performed on the rurual clinic level for 6 years of data. This is a type of hierarchical regression.\n\r\r\rIf the number of variables is very large compared to observations (p\u0026gt;n), for example genomics, a person has hundreds of genes. Or when the ratio of p/n is larger than normal and the linear/nonlinear relationships and assumptions are vague, noval machine learning methods are preferred.\nOne example is the breast cancer tumor classification. Another example is a Leukemia project that i’m currently working on to identify unknown gene mutation effects to the mortality of the patients. There are only 125 patients, and each patient has over 38 gene mutations. The gene mutations are sparse. Methods with penalty and constraints will be suitable for this type of data. I’ll discuss more about this project seperately later.\n\rPrediction: Predict outcomes. It preferrs big sample size for better prediction accuracy.\n\rCovid 19 Study This paper was reference for prediciton: Due to the extensive research studies on Covid 19. Our hospital identified various data and interesting risk factors to predict Covid 19 positive cases. On one hand, the study aims to identify additional risk factors. and on the other hand, with over 10K patients’ data, the study aims to predict Covid 19 cases based on the massive data. Multiple logistic regression, Random Forest, and XGboost were used to predict the outcome. Since the risk factors and response variable have more linear relationship, and with a better interpretability, Multiple logistic regression with training and validation test was picked and each patient has a risk score for decision makers to utilize the hospital resources.\r\r\rClosing Note\nIn healthcare research, asking the right questions and have clinical knowledge is very essential to determine the patient population and appropriate methods. Understanding the problems and using the efficient methods provides a strong solution. Statistical inference is essential in traditional Health care research. Maching learning method is more flexible and is generally better for prediction, big data or unknown assumptions.\n","date":1596067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596162449,"objectID":"3166aa32d494881b8aa99673dd4f9bf1","permalink":"https://yuan-du.com/post/2020-07-30-statvsml/statisticsvsml/","publishdate":"2020-07-30T00:00:00Z","relpermalink":"/post/2020-07-30-statvsml/statisticsvsml/","section":"post","summary":"There are so many terms regarding the field of Statistics and Data Science. We often heard Statistics, Data Mining, Machine Learning, Big Data, etc. It especially confuses people that’s in a different field. I remember that over five years ago, a radiologist asked me if I can mine data from the radiology system because she saw that I have Data Mining skills. I was blown away by the understanding of Data Mining to a doctor.","tags":["Statistical Learning","Statistical Modeling","Machine Learning","Data Science"],"title":"Research Experience - Comparing Statistics vs Machine Learning","type":"post"},{"authors":["Luwei Tao","Ruoyu Miao","Tarek Mekhail","Jingxin Sun","Lingbin Meng","Cheng Fang","Jian Guan","Akriti Jain","Yuan Du","Amanda Allen","Brenda L Rzeszutko","Mark A Socinski","Chung-Che Chang"],"categories":null,"content":"","date":1594771200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594771200,"objectID":"f1117b8b02105dc551c28c611aab9bda","permalink":"https://yuan-du.com/publication/2020-7-clc/","publishdate":"2020-07-15T00:00:00Z","relpermalink":"/publication/2020-7-clc/","section":"publication","summary":"KRAS mutation subtype is not associated with patient clinical outcomes or PD-L1 expression status. However, PD-L1 positivity appears to negatively affect OS in LADC patients with G12C mutation. Further study is needed to confirm our observation and to determine if programmed cell death 1/PD-L1 antagonist may affect the clinical outcome of patients with different KRAS mutation subtypes.","tags":["Lung Caner","Gene","Survival","Driver mutation","Immunotherapy","Molecular profiling","Precision medicine","Prognosis"],"title":"Prognostic Value of KRAS Mutation Subtypes and PD-L1 Expression in Patients With Lung Adenocarcinoma","type":"publication"},{"authors":["Mamoon Ur Rashid","Neelam khetpal","Hammad Zafar","Saeed Ali","Evgeny Idrisov","Yuan Du","Assaf Stein","Deepanshu Jain","Muhammad Khalid Hasan"],"categories":null,"content":"","date":1594080000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594080000,"objectID":"1a3518c90f1b75b3c7b2fd818727de39","permalink":"https://yuan-du.com/publication/2020-7-wjge/","publishdate":"2020-07-07T00:00:00Z","relpermalink":"/publication/2020-7-wjge/","section":"publication","summary":"There was overall low prevalence of SMIL in our study. Kudo pit pattern (IIIL + IV and V) and Paris classification 0-IIc were the only factors identified as an independent risk factor for submucosal invasion. The independent risk factor for recurrence was adenoma size ( 40 mm). Almost all recurrences (98.8%) were treated endoscopically.","tags":["EMR","SMI","Logistic regression"],"title":"Colon mucosal neoplasia referred for endoscopic mucosal resection: Recurrence of adenomas and prediction of submucosal invasion","type":"publication"},{"authors":[],"categories":["Statistics","R"],"content":"\rBayesian approach becomes more and more popular because of the improvement of the mordern computing ability for machine learning and big data. Bayesian analysis is a completely different approach compared to frequentist approach. Yet, it’s more challenging. To be able to understand and learn the Bayesian approach, first, we will need to have the knowledge of conditional probability. Second, to have the knowledge of different distributions such as normal, bernoulli, binomial, gamma, beta, cauchy, possion, etc. Third, to be familiar with calculus. We need to calculate derivatives and integration of different distributions. Last but not at least, to be familiar with simulation sampling techniques such as Grid sampling, Variational Bayes and Monte Carlo Markov Chian (MCMC) including popular Gibbs sampling, Metropolis Hastings Sampling, Hamiltonian Monte Carlo (HMC),etc. Luckily, there are a few software tools Jags, Stan .etc can be used for Gibbs sampling and HMC.\rThe concept is simple. According to Bayes Theorem \\(p(\\theta|y) = \\frac{p(y|\\theta)p(\\theta)}{p(y)}\\) or without normalization \\(p(\\theta|y) \\sim p(y|\\theta)p(\\theta)\\), we want to gain the posterior distribution by using prior information and likelihood of the data information. Most of the time, we use log likelihood instead for easier calculation since \\(log( a*b )= loga +logb\\). Bayesian approach involves much more math than frequentist approach and more complicated. Frequentist focus on one point estimate (hypothesis and Confidence Interval), and bayesian focuses on a looking forward range (Credible Interval).\rIf friquentist and bayesian approach are just two different ways to look into things, Why and when is recommended to use Bayesian approach instead of frequentist approach?\nHere are a few examples to use Bayesian approach over frequentist approach:\n\rClear prior information: the example in the book BD3 of Andrew Gelman’s in Chapter 1, problem 6. The prior information is that approximately 1/125 of all births are fraternal twins and 1/300 of births are identical twins.\n\rSeperation problems. For example logistic regression couldn’t converge due to high dimension and small sample size.\n\rEstimate multiple outcomes with credible interval. For example, family doctors try to diagnosis diseases (such as cold, flu) based on multiple symptoms (such as headache, sore throat, high temprature) and the probabilities of all symptoms sum up to limited possible diseases.\n\rSmall sample size with multiple experiments and limited budget.It’s a preferred meta analysis than tranditional meta analysis that has high heterogeneity with different recources since it provides a credible interval instead of confident interval.\n\r\rReference:\nDicing with the unknown\n\rFDA Guidance for the Use of Bayesian Statistics in Medical Device Clinical Trials\n\r\r","date":1587168000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587263249,"objectID":"6412b287a67783d77602d81a8ed49653","permalink":"https://yuan-du.com/post/2020-04-18-bayesian-analysis/bayesian-analysis/","publishdate":"2020-04-18T00:00:00Z","relpermalink":"/post/2020-04-18-bayesian-analysis/bayesian-analysis/","section":"post","summary":"Bayesian approach becomes more and more popular because of the improvement of the mordern computing ability for machine learning and big data. Bayesian analysis is a completely different approach compared to frequentist approach. Yet, it’s more challenging. To be able to understand and learn the Bayesian approach, first, we will need to have the knowledge of conditional probability. Second, to have the knowledge of different distributions such as normal, bernoulli, binomial, gamma, beta, cauchy, possion, etc.","tags":["Bayesian","Statistical Modeling","Machine Learning","MCMC"],"title":"Bayesian Analysis","type":"post"},{"authors":[],"categories":["Statistics"],"content":"\rIt’s been a hectic few months since my last post. Working and schooling at home during this pandemic chaos is challenging, but it doesn’t stop the hope of renew and transformation. One of the most popular statistcal modeling method for epidemiology and network science-Spreading Phenomena is SIR (susceptible-infected-recovered) model from Tim Churches.\nSusceptible (S): Healthy individuals who have not yet contacted the pathogen.\nInfectious (I): Contagious individuals who have contacted the pathogen and hence can infect others.\nRecovered/Removed (R): Infected individuals become immune or die, i.e., will not be infected again and cannot infect anyone else.\rAs shown in the picture above, there are two important rates \\(\\beta\\)(transmission rate of the pathogen), \\(\\gamma\\)(recovery rate). You probablily heard of the basic reproduction number \\(R_0\\) value (R-nought), which can be calculated by \\(\\frac{\\beta}{\\gamma}\\).\nIf \\(R_0\\) \u0026gt; 1, epidemic is in the epidemic state;\nIf \\(R_0\\) \u0026lt; 1, epidemic dies out.\nThe purpose of the post is to estiamte the basic reproduction number R-nought / \\(R_0\\) value based on US data and then make predictions for the future.\n1. Load packages:\n#install.packages(c(\u0026quot;httr\u0026quot;, \u0026quot;jsonlite\u0026quot;))\rlibrary(httr)\rlibrary(jsonlite)\r#install.packages(\u0026quot;optimr\u0026quot;)\rlibrary(optimr)\rlibrary(deSolve)\rlibrary(tidyverse)\rlibrary(lubridate)\r2. Retrieve data from John’s Hopkins and use date March 1st till March 15 to fit SIR Model and check if the fit is reasonable:\n#by Johns Hopkins CSSE\rres = GET(\u0026quot;https://pomber.github.io/covid19/timeseries.json\u0026quot;)\rdata = fromJSON(rawToChar(res$content))\r#names(data) all contries US = data$US\rUS$date=as.Date(US$date) new\u0026lt;-US %\u0026gt;% dplyr::filter(date\u0026gt;= \u0026quot;2020-03-01\u0026quot;\u0026amp; date\u0026lt;\u0026quot;2020-03-15\u0026quot;) #for US data only\rhead(new) #first six rows\r## date confirmed deaths recovered\r## 1 2020-03-01 32 1 7\r## 2 2020-03-02 54 6 7\r## 3 2020-03-03 74 7 7\r## 4 2020-03-04 107 11 7\r## 5 2020-03-05 184 12 7\r## 6 2020-03-06 237 14 7\rtail(new) #last six rows\r## date confirmed deaths recovered\r## 9 2020-03-09 589 22 7\r## 10 2020-03-10 782 28 8\r## 11 2020-03-11 1145 33 8\r## 12 2020-03-12 1584 43 12\r## 13 2020-03-13 2214 51 12\r## 14 2020-03-14 2971 58 12\r3. build SIR Model and find \\(\\beta\\) and \\(\\gamma\\) by optimization method RSS:\n#Initial values:\rInfected \u0026lt;- new$confirmed\rDay \u0026lt;- 1:(length(new$date))\rN\u0026lt;-327200000 #US population\rinit \u0026lt;- c(S = N - Infected[1], I = Infected[1], R = 0) #Inital value\r## SIR Model\rSIR \u0026lt;- function(time, state, parameters) {\rpar \u0026lt;- as.list(c(state, parameters))\rwith(par, {\rdS \u0026lt;- -beta * I * S/N\rdI \u0026lt;- beta * I * S/N - gamma * I\rdR \u0026lt;- gamma * I\rlist(c(dS, dI, dR))\r})\r}\r#Optimization by RSS to get beta and gamma\rRSS \u0026lt;- function(parameters) {\rnames(parameters) \u0026lt;- c(\u0026quot;beta\u0026quot;, \u0026quot;gamma\u0026quot;)\rout \u0026lt;- ode(y = init, times = Day, func = SIR, parms = parameters)\rfit \u0026lt;- out[, 3]\rsum((Infected - fit)^2)\r}\rOpt \u0026lt;- optim(c(0.5, 0.5), RSS, method = \u0026quot;L-BFGS-B\u0026quot;, lower = c(0,0), upper = c(1, 1))\rOpt_par \u0026lt;- setNames(Opt$par, c(\u0026quot;beta\u0026quot;, \u0026quot;gamma\u0026quot;))\rOpt_par\r## beta gamma ## 0.6755947 0.3244053\r#R0\rOpt$par[1]/Opt$par[2]\r## [1] 2.082563\r4. Plot the predicted value vs raw data\nsir_start_date \u0026lt;- \u0026quot;2020-03-01\u0026quot;\rt \u0026lt;- 1:as.integer(ymd(\u0026quot;2020-03-15\u0026quot;) - ymd(sir_start_date))\r# get the fitted values from our SIR model\rfitted_cumulative_incidence \u0026lt;- data.frame(ode(y = init, times = t, func = SIR, parms = Opt_par))\r# add a Date column and join the observed incidence data\rfitted_cumulative_incidence \u0026lt;- fitted_cumulative_incidence %\u0026gt;% mutate(date = ymd(sir_start_date) + days(t - 1)) %\u0026gt;% left_join(new %\u0026gt;% ungroup() %\u0026gt;% select(date, confirmed))\r## Joining, by = \u0026quot;date\u0026quot;\r# plot the data\rfitted_cumulative_incidence %\u0026gt;% ggplot(aes(x = date)) + geom_line(aes(y = I), colour = \u0026quot;red\u0026quot;) + geom_point(aes(y = confirmed), colour = \u0026quot;orange\u0026quot;) + labs(y = \u0026quot;Cumulative incidence\u0026quot;, title = \u0026quot;COVID-19 fitted vs observed cumulative incidence, US 03/01-03/15\u0026quot;, subtitle = \u0026quot;(red=fitted incidence from SIR model, orange=observed incidence)\u0026quot;)\r5. The fit looks reasonable with R0=1.78, now we use the model to predict the curve for 3 months starting from March\n#Prediction\r# time in days for predictions\rt \u0026lt;- 1:90\r# get the fitted values from our SIR model\rfitted_cumulative_incidence \u0026lt;- data.frame(ode(y = init, times = t, func = SIR, parms = Opt_par))\r# add a Date column and join the observed incidence data\rfitted_cumulative_incidence \u0026lt;- fitted_cumulative_incidence %\u0026gt;% mutate(date = ymd(sir_start_date) + days(t - 1)) %\u0026gt;% left_join(new %\u0026gt;% ungroup() %\u0026gt;% select(date, confirmed))\r## Joining, by = \u0026quot;date\u0026quot;\r# plot the data\rfitted_cumulative_incidence %\u0026gt;% ggplot(aes(x = date)) + geom_line(aes(y = I), colour = \u0026quot;red\u0026quot;) + geom_line(aes(y = S), colour = \u0026quot;black\u0026quot;) + geom_line(aes(y = R), colour = \u0026quot;green\u0026quot;) + geom_point(aes(y = confirmed), colour = \u0026quot;orange\u0026quot;) + scale_y_continuous(labels = scales::comma) + labs(y = \u0026quot;Persons\u0026quot;, title = \u0026quot;COVID-19 3 months prediction\u0026quot;) + scale_colour_manual(name = \u0026quot;\u0026quot;, values = c(red = \u0026quot;red\u0026quot;, black = \u0026quot;black\u0026quot;, green = \u0026quot;green\u0026quot;, orange = \u0026quot;orange\u0026quot;), labels = c(\u0026quot;Susceptible\u0026quot;, \u0026quot;Recovered\u0026quot;, \u0026quot;Observed incidence\u0026quot;, \u0026quot;Infectious\u0026quot;))\rConclusion: It looks like the peak will be in the end of April and it will die down at the end of May.\n","date":1587168000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587263754,"objectID":"73e0f8a7f82723ae53245e267ed50a74","permalink":"https://yuan-du.com/post/2020-04-17-sir-model/2020-04-18-sir-model-for-covid19/","publishdate":"2020-04-18T00:00:00Z","relpermalink":"/post/2020-04-17-sir-model/2020-04-18-sir-model-for-covid19/","section":"post","summary":"It’s been a hectic few months since my last post. Working and schooling at home during this pandemic chaos is challenging, but it doesn’t stop the hope of renew and transformation. One of the most popular statistcal modeling method for epidemiology and network science-Spreading Phenomena is SIR (susceptible-infected-recovered) model from Tim Churches.\nSusceptible (S): Healthy individuals who have not yet contacted the pathogen.\nInfectious (I): Contagious individuals who have contacted the pathogen and hence can infect others.","tags":["Statistics","Statistical Modeling","Epidemiology","Network Science","Covid19"],"title":"SIR Model for Covid19","type":"post"},{"authors":["Patricia Louzon","Jessica Andrews","Xavier Torres","Eric Pyles","Mahmood Ali","Yuan Du","John Devlin"],"categories":null,"content":"","date":1586131200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586131200,"objectID":"7fc065ba0380366825dfd371890efd48","permalink":"https://yuan-du.com/publication/2020-4-pharm/","publishdate":"2020-04-24T00:00:00Z","relpermalink":"/publication/2020-4-pharm/","section":"publication","summary":"A Fitbit Charge 2 when applied to non-intubated adults in an ICU consistently collects TST data but not","tags":["Fitbit","sleep quality","ICU","correlation","Bootstrap","Pharmacy"],"title":"Characterisation of ICU sleep by a commercially available activity tracker and its agreement with patient-perceived sleep quality","type":"publication"},{"authors":null,"categories":null,"content":"Review the Short Version of the Presentation \nSummary: This result provided the best solution for the issue of the increasing members\u0026rsquo; churn rate for AFCU (Addition Financial Credit Union) Churn Analytics Competition, and was reward as the 1st Place Winner in 2020.\nThis analytical approach based on transation activities could be also applied to similar problems in a Healthcare setting, such as predicting patient outcomes by prescription activities, predicting wellness outcomes or status by digital health activities (fitness, sleep), etc.\n","date":1582675200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582675200,"objectID":"6eef192f97085214bcead31cbbea2015","permalink":"https://yuan-du.com/project/churn-project/","publishdate":"2020-02-26T00:00:00Z","relpermalink":"/project/churn-project/","section":"project","summary":"Data Analytics Competition using customers information and historical transaction activities to predict if a customer is about to churn.","tags":["Machine Learning"],"title":"Customer Churn Project - 1st Place winner","type":"project"},{"authors":[],"categories":null,"content":"Breast Cancer Image Classification Analysis Mainly by SVM\n Click on the report button above to view the built-in slides feature.   ","date":1577291400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577291400,"objectID":"929a50445c232032e2e4b72bdc3a0fd0","permalink":"https://yuan-du.com/talk/2019-11-25-breast-cancer-by-svm/","publishdate":"2019-10-14T00:00:00Z","relpermalink":"/talk/2019-11-25-breast-cancer-by-svm/","section":"talk","summary":"Breast Cancer Image Classification Analysis Mainly by SVM.","tags":["Asymptotic Theory","Theorem","SVM","Statistical Modeling","Machine Learning","Statistics"],"title":"Breast Cancer Image Classification Analysis by SVM","type":"talk"},{"authors":[],"categories":null,"content":"This is an Advanced Statistical course followed by the Statistical Basics. The most commonly used tests are introduced in this class for general research. I highly recommend that you attend the Statistical Basics before attending this course.\n Click on the demo slides button above to view the built-in slides feature.   ","date":1573119000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573119000,"objectID":"d8bcc0020f272b06b961af2a55979ac7","permalink":"https://yuan-du.com/talk/2019-09-advancedstat/","publishdate":"2019-09-14T00:00:00Z","relpermalink":"/talk/2019-09-advancedstat/","section":"talk","summary":"This is an Advanced Statistical course followed by the Statistical Basics. The most commonly used tests are introduced in this class for general research. I highly recommend that you attend the Statistical Basics before attending this course.","tags":["Hypothesis testing","Statistics"],"title":"Hypothesis Testing","type":"talk"},{"authors":[],"categories":null,"content":"This is a SVM Asymptotic normality review of the article \u0026ldquo;Asymptotic normality of support vector machine variants and other regularized kernel methods\u0026rdquo;.\n Click on the slides button above to view the built-in slides feature.   ","date":1572971400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572971400,"objectID":"d07e7ecddc4b587d0b14bf31a1beeb51","permalink":"https://yuan-du.com/talk/2019-10-svm/","publishdate":"2019-10-14T00:00:00Z","relpermalink":"/talk/2019-10-svm/","section":"talk","summary":"This is a SVM Asymptotic normality review of the article \"Asymptotic normality of support vector machine variants and other regularized kernel methods\".","tags":["Asymptotic Theory","Theorem","SVM","Statistics"],"title":"SVM Asymptotic properties review","type":"talk"},{"authors":[],"categories":["Statistics"],"content":"\rMachine learning algorithms are very populuar. However, machine learing algorithms are not stable/consistant on the performance because lots of them are not using statistical inference. Thus, statistical theory for estimating function which has established hundreds of years ago becomes a more and more interesting research direction.\nIn this blog, I will introduce a few important asymptotic theorems that are fundamental to prove some machine learning algorithms, such as SVM and Markov Chain.\nFatou-Lebesgue Lemma:\rif the random variable \\(X_n \\xrightarrow{a.s} X\\) and if for all n \\(X_n \\geq Y\\) with \\(E|Y| \u0026lt; \\infty\\), then\r\\[E(\\liminf_{n \\to \\infty} X_n) \\leq \\liminf_{n \\to \\infty} E(X_n)\\]\rIt holds if \\(X_n \\geq 0\\) for all n. By using Fatou-Lebesgue Lemma, we can prove the (a) Monotone convergence Theorem, and the (b) Lebesgue Dominated Convergence Theorem.\n\r(a) Monotone convergence Theorem: If \\(X_n\\) is a sequence of nonnegative measurable functions denoted by \\(0 \\leq x_1 \\leq x_2 \\dots \\leq x_n \\leq x_{n+1}\\) and \\(X_n \\xrightarrow{a.s} X\\), then \\(\\lim_{0\\to\\infty}E(X_n)=E(\\lim_{0\\to\\infty}X_n) = EX\\)\n\r(b) Lebesgue Dominated Convergence Theorem：If the random variables \\(X_n \\to X\\), then we have \\(|X_n| \\leq Y\\), almost surely for all n. Then \\(X_n \\in L^1\\), \\(X \\in L^1\\), and \\(\\lim_{0\\to\\infty}E(X_n) = E(X)\\).\n\r\rPartical converge relation：\\(X_n \\xrightarrow{P}X\\) if and only if every subsequence \\(n_1, N_2, \\dots \\epsilon \\{1,2,\\dots\\}\\) has a sub-sequence \\(m_1, m_2, \\dots \\epsilon \\{n_1,n_2,\\dots \\}\\) such that \\(X_{m_j} \\xrightarrow{a.s}X\\) as \\(j\\to\\infty\\).\nBorel-Cantelli Lemma: for {\\(A_n: n \\geq 1\\)} a sequence of events in a probability space if \\(\\sum_{n=1}^{\\infty}P(A_n) \u0026lt; \\infty\\) then \\(P(A_n i.o.)=0\\); only a finite number of the events occur, with probability 1. Conversely, if the \\(A_n\\) are independent and \\(\\sum_{n=1}^{\\infty}P(A_n) = \\infty\\), then \\(P(A_n i.o.)=1\\); an infinite number of the events occur, with probability 1.\nBorel-Cantelli Lemma is useful in problems related to the a.s. convergence. It could be written as \\(P(|X_n - X|\u0026gt;\\epsilon i.o.) = 0, \\forall \\epsilon \u0026gt; 0\\).\nLaws of Large Numbers: When the convergence is in probability or law, this is known as weak law of large numbers (WLLN). if \\(E|X| \u0026lt; \\infty\\), then \\(\\bar{X_n} \\xrightarrow{P} \\mu = EX\\). When the convergence is almost surely, it is the strong laws of large nubmers (SLLN). \\(\\bar{X_n} \\xrightarrow{a.s.} \\mu \\Leftrightarrow EX \u0026lt; \\infty\\) and \\(\\mu = EX\\)\nCentral Limit Theorems: Let \\(X_1,X_2,\\dots\\) be i.i.d. random vectors with mean \\(\\mu\\) and finite covariance matrix, \\(\\Sigma\\). Then \\(\\sqrt{n}(\\bar{X_n} - \\mu) \\xrightarrow{L} N(0,\\Sigma)\\).\nSlutsky’s Theorem: Let \\(\\{X_n\\}, \\{Y_n\\}\\) be sequences of scalar/vector/matrix random elements. If \\(X_n\\) converges in distribution to a random element X; and \\(Y_n\\) converges in probability to a constant c, then \\(X_n + Y_n \\xrightarrow{d} X + c\\), \\(X_nY_n \\xrightarrow{d} cX\\), \\(\\frac{X_n}{Y_n} \\xrightarrow{d} \\frac{X_n}{c}\\).\nSVM1could be an application of Lebesgue Dominated Convergence Theorem and Central Limit Theorem. We can use the theorem and Partical converge relation to prove the hinge loss function, when the data is not linearly separable. By limiting on Hilbert space, a weakly convergent subsequence. We can apply asymptotic normality property on the regularization parameter \\(\\lambda_i \\to 0\\) and thus to solve the miminization problem on the hyperplane \\(\\omega_n\\), where the solution \\(\\tilde{\\omega}= \\omega_*\\) because \\(\\omega_*(\\lambda_i) \\xrightarrow{a.s.} \\omega_*\\).\nThe Markov chain2 can be proved by using Borel-Cantelli Lemma. The probability of having state from \\(i\\) and eventually return to \\(i\\) is 1. If this probability is strictly less than 1, \\(i\\) is called transient.\n\rSVM is supervised learning model with associated learning algorithm that analyzes data used for classification and regression analysis.↩\n\rA Markov chain is a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event. In probability theory and related fields, a Markov process, named after the Russian mathematician Andrey Markov, is a stochastic process that satisfies the Markov property. The defining characteristic of a Markov chain is that no matter how the process arrived at its present state, the possible future states are fixed. In other words, the probability of transitioning to any particular state is dependent solely on the current state and time elapsed. The state space, or set of all possible states, can be anything: letters, numbers, weather conditions, sales volume，etc.↩\n\r\r\r","date":1569628800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569628800,"objectID":"23036f800c3bce922da345e4680a168e","permalink":"https://yuan-du.com/post/2019-09-28-asymtotic-theory/important-asymtotic-theorems/","publishdate":"2019-09-28T00:00:00Z","relpermalink":"/post/2019-09-28-asymtotic-theory/important-asymtotic-theorems/","section":"post","summary":"Machine learning algorithms are very populuar. However, machine learing algorithms are not stable/consistant on the performance because lots of them are not using statistical inference. Thus, statistical theory for estimating function which has established hundreds of years ago becomes a more and more interesting research direction.\nIn this blog, I will introduce a few important asymptotic theorems that are fundamental to prove some machine learning algorithms, such as SVM and Markov Chain.","tags":["Asymptotic Theory","Theorem","MCMC","SVM"],"title":"Important asymptotic theorems","type":"post"},{"authors":null,"categories":["R","Tools"],"content":"Recently, I discovered a few pretty cool tools, R-based and easy to follow.\n R Presentation Themes\n Why uses R presentation?\n Take advantage of R Markdown . You can write all your slides in Markdown text Power of interchange. You can include chunks of R code and rendered output like plots, results, tables, etc. in your slides Version control and sharing. You can use git for version control and share your GitHub repository  My current favorate Xaringan nanja-theme was developed by Emi Tanaka.\n  Data Cleaning/Data Wrangling tool kit - Tidyverse\n Book R for Data Science by Garrett Grolemund \u0026amp; Hadley Wickham.\n Visual Data cleaning post- Tidyverse overview by Julia Stewart Lowndes.\n   I\u0026rsquo;m finishing up on an advanced statistical class \u0026ldquo;Significance Test\u0026rdquo; by using R presentation and will share the code on Github. so stay tuned\u0026hellip;\n","date":1565913600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565913600,"objectID":"b2b08b2f287393feb499ef74c86b652a","permalink":"https://yuan-du.com/post/tools-reference/","publishdate":"2019-08-16T00:00:00Z","relpermalink":"/post/tools-reference/","section":"post","summary":"Recently, I discovered a few pretty cool tools, R-based and easy to follow.\n R Presentation Themes\n Why uses R presentation?\n Take advantage of R Markdown . You can write all your slides in Markdown text Power of interchange. You can include chunks of R code and rendered output like plots, results, tables, etc. in your slides Version control and sharing. You can use git for version control and share your GitHub repository  My current favorate Xaringan nanja-theme was developed by Emi Tanaka.","tags":["Tidyverse","Data Wrangling","Data Cleaning","Statistical Modeling","Machine Learning","Tools","Reference"],"title":"Awesome data science presentations and tools","type":"post"},{"authors":null,"categories":[],"content":"Curiosity\nI\u0026rsquo;ve always been a person with a curious mind that constantly searches and looks for the truth. The wonder of history and the future, and pursuing on the insights drives me to the field of Statistics. I didn\u0026rsquo;t choose this field but I was accidentally inspired to be in the field. It was difficult for me to understand all the mathmatics and it\u0026rsquo;s still difficult till this day. But the more I know, the better I understand that how much I don\u0026rsquo;t know.\nPowerlessness\nBeing pressured to achieve in this world from society has always made me self-doubt. Accepting the truth of human weakness and knowing that I\u0026rsquo;m not perfect and I don\u0026rsquo;t need to be perfect helps me to identify myself and be humble. Data Science is the emerging field that requries multidisciplinary knowledge and meanwhile creates different level of opportunities for everyone. You don\u0026rsquo;t need a fancy PhD to be a Data Scientist. The unique background and learning apitude makes a better scientist. Embracing the powerlessness as a human is the power that drives people to grow in a healthy way.\nCompassion\nThe wilingness to make a better decision and to make a better world through love and passion pushes me to go above and beyond and move forward. Our daily life is affected by technology heavily. and there are still a lot of challenges that we want to make an improvement on. My grandma from my dad side passed away from stamoch cancer, my grandma from my mom side passed away from acute pancreatitis. It was a tough time for me to watch my grandma in the ICU and breathed on the ventilation machine. I had the fear of being in the hospital and somehow I ended up in healthcare research and work now as a Biostatistician in the hospital. My knowledge of medicine and healthcare over the years has increased dramatically and there is still so much to learn about medical terminology, clinical process, insurance, billing, coding, policy, etc.\nFaith\nFaith is being sure of what we hope for and certain of what we do not see. Especially during this pandemic, you see the fears, the chaos, the riots, the unjustice, the hypocrisy, the love and the hate. I\u0026rsquo;m very grateful to survive and thrive during the difficult time. This chaotic event makes people realize how weak we are as human beings. Scientific research becomes more and more important to help with medical advancement and knowledge improvement. I believe the ultimate solution of this unclear state of world is \u0026ldquo;Faith, Hope and Love.and the greatest of these is Love\u0026rdquo; .\n","date":1564444800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564444800,"objectID":"39d643ac0d0009f7e6d68da32eb416e2","permalink":"https://yuan-du.com/post/my-journey-of-being-research-statistician/","publishdate":"2019-07-30T00:00:00Z","relpermalink":"/post/my-journey-of-being-research-statistician/","section":"post","summary":"Curiosity\nI\u0026rsquo;ve always been a person with a curious mind that constantly searches and looks for the truth. The wonder of history and the future, and pursuing on the insights drives me to the field of Statistics. I didn\u0026rsquo;t choose this field but I was accidentally inspired to be in the field. It was difficult for me to understand all the mathmatics and it\u0026rsquo;s still difficult till this day. But the more I know, the better I understand that how much I don\u0026rsquo;t know.","tags":["Statistician","Biostatistician","Data Scientist","Research"],"title":"My journey of being a research statistician","type":"post"},{"authors":[],"categories":["R","Text Mining"],"content":"\rThanks to the short course at SDSS 2019, I learned how to do tf-idf to topic modeling and sentiment analysis by using tidytext taught by Julia Silge, author of Text Mining with R and Mara Averick. They did a great job on teaching the four hour class. I didn’t expect to have so much covered in the short course.\nHere is an example that I used the method to analyze A Tale of Two Cities and Great Expectations by Charles Dickens by using sentiment analysis.\nInstall the tidytext package for text mining.\r\rinstall.packages(\"tidytext\")\nRead the book from gutenbergr package, after install gutenbergr package\r\rWe can Downloading books by ID (98 and 1400) from Project Gutenbergr.\nlibrary(gutenbergr)\rlibrary(dplyr)\rbook \u0026lt;- gutenberg_download(c(98, 1400), meta_fields = \u0026quot;title\u0026quot;) %\u0026gt;%\rgroup_by(title) %\u0026gt;%\rmutate(line = row_number()) %\u0026gt;%\rungroup()\rbook\r## # A tibble: 35,889 x 4\r## gutenberg_id text title line\r## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt;\r## 1 98 A TALE OF TWO CITIES A Tale of Two Cities 1\r## 2 98 \u0026quot;\u0026quot; A Tale of Two Cities 2\r## 3 98 A STORY OF THE FRENCH REVOLUTION A Tale of Two Cities 3\r## 4 98 \u0026quot;\u0026quot; A Tale of Two Cities 4\r## 5 98 By Charles Dickens A Tale of Two Cities 5\r## 6 98 \u0026quot;\u0026quot; A Tale of Two Cities 6\r## 7 98 \u0026quot;\u0026quot; A Tale of Two Cities 7\r## 8 98 CONTENTS A Tale of Two Cities 8\r## 9 98 \u0026quot;\u0026quot; A Tale of Two Cities 9\r## 10 98 \u0026quot;\u0026quot; A Tale of Two Cities 10\r## # ... with 35,879 more rows\rProcess books into chapters and words in tidy data form\r\rwe need to restructure it as one-token-per-row format. As pre-processing, we divide these into chapters, use tidytext’s unnest_tokens to separate them into words, then remove stop_words. We’re treating every chapter as a separate “document”, each with a name like A Tale of Two cities or Great Expectations.\nlibrary(tidytext)\rtidy_book \u0026lt;- book %\u0026gt;%\runnest_tokens(word, text)\rtidy_book\r## # A tibble: 323,972 x 4\r## gutenberg_id title line word ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; ## 1 98 A Tale of Two Cities 1 a ## 2 98 A Tale of Two Cities 1 tale ## 3 98 A Tale of Two Cities 1 of ## 4 98 A Tale of Two Cities 1 two ## 5 98 A Tale of Two Cities 1 cities\r## 6 98 A Tale of Two Cities 3 a ## 7 98 A Tale of Two Cities 3 story ## 8 98 A Tale of Two Cities 3 of ## 9 98 A Tale of Two Cities 3 the ## 10 98 A Tale of Two Cities 3 french\r## # ... with 323,962 more rows\rtidy_book \u0026lt;- tidy_book %\u0026gt;%\ranti_join(get_stopwords())\r#We can also use count to find the most common words in all the book as a whole\rtidy_book %\u0026gt;%\rcount(word, sort = TRUE)\r## # A tibble: 14,594 x 2\r## word n\r## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt;\r## 1 said 2010\r## 2 mr 1333\r## 3 one 940\r## 4 now 715\r## 5 joe 698\r## 6 upon 655\r## 7 time 640\r## 8 little 638\r## 9 miss 616\r## 10 know 613\r## # ... with 14,584 more rows\rSentiment analysis\r\rSentiment analysis can be done as an inner join. Three sentiment lexicons are available via the get_sentiments() function. Let’s examine how sentiment changes during each novel. Let’s find a sentiment score for each word using the Bing lexicon, then count the number of positive and negative words in defined sections of each novel.\nlibrary(tidyr)\rget_sentiments(\u0026quot;bing\u0026quot;)\r## # A tibble: 6,786 x 2\r## word sentiment\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 2-faces negative ## 2 abnormal negative ## 3 abolish negative ## 4 abominable negative ## 5 abominably negative ## 6 abominate negative ## 7 abomination negative ## 8 abort negative ## 9 aborted negative ## 10 aborts negative ## # ... with 6,776 more rows\rsentiment \u0026lt;- tidy_book %\u0026gt;%\rinner_join(get_sentiments(\u0026quot;bing\u0026quot;), by = \u0026quot;word\u0026quot;) %\u0026gt;% count(title, index = line %/% 80, sentiment) %\u0026gt;% spread(sentiment, n, fill = 0) %\u0026gt;% mutate(sentiment = positive - negative)\rsentiment\r## # A tibble: 450 x 5\r## title index negative positive sentiment\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 A Tale of Two Cities 0 5 9 4\r## 2 A Tale of Two Cities 1 28 28 0\r## 3 A Tale of Two Cities 2 27 15 -12\r## 4 A Tale of Two Cities 3 17 10 -7\r## 5 A Tale of Two Cities 4 13 12 -1\r## 6 A Tale of Two Cities 5 19 18 -1\r## 7 A Tale of Two Cities 6 24 16 -8\r## 8 A Tale of Two Cities 7 24 19 -5\r## 9 A Tale of Two Cities 8 9 27 18\r## 10 A Tale of Two Cities 9 29 23 -6\r## # ... with 440 more rows\rNow we can plot these sentiment scores across the plot trajectory of each novel.\nlibrary(ggplot2)\rggplot(sentiment, aes(index, sentiment, fill = title)) +\rgeom_bar(stat = \u0026quot;identity\u0026quot;, show.legend = FALSE) +\rfacet_wrap(~title, ncol = 2, scales = \u0026quot;free_x\u0026quot;)\rIt looks like that A Table of Two Cities has more negative emotions and Great Expectations is more balanced on emotions.\nMost common positive and negative words\r\rOne advantage of having the data frame with both sentiment and word is that we can analyze word counts that contribute to each sentiment.\nbing_word_counts \u0026lt;- tidy_book %\u0026gt;%\rinner_join(get_sentiments(\u0026quot;bing\u0026quot;)) %\u0026gt;%\rcount(word, sentiment, sort = TRUE)\rbing_word_counts\r## # A tibble: 2,575 x 3\r## word sentiment n\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt;\r## 1 miss negative 616\r## 2 like positive 541\r## 3 well positive 483\r## 4 good positive 473\r## 5 great positive 360\r## 6 better positive 221\r## 7 right positive 172\r## 8 poor negative 164\r## 9 dark negative 160\r## 10 work positive 152\r## # ... with 2,565 more rows\rThis can be shown visually, and we can pipe straight into ggplot2 because of the way we are consistently using tools built for handling tidy data frames.\nbing_word_counts %\u0026gt;%\rfilter(n \u0026gt; 100) %\u0026gt;%\rmutate(n = ifelse(sentiment == \u0026quot;negative\u0026quot;, -n, n)) %\u0026gt;%\rmutate(word = reorder(word, n)) %\u0026gt;%\rggplot(aes(word, n, fill = sentiment)) +\rgeom_col() +\rcoord_flip() +\rlabs(y = \u0026quot;Contribution to sentiment\u0026quot;)\rThis lets us spot an anomaly in the sentiment analysis; the word “miss” is coded as negative but it is used as a title for young, unmarried women in Jane Austen’s works. If it were appropriate for our purposes, we could easily add “miss” to a custom stop-words list using bind_rows.\ncustom_stop_words \u0026lt;- bind_rows(get_stopwords(),\rtibble(word = \u0026quot;miss\u0026quot;,\rlexicon = \u0026quot;custom\u0026quot;))\rtidy_book2 \u0026lt;- tidy_book %\u0026gt;%\ranti_join(custom_stop_words) %\u0026gt;%\rcount(word, sort = TRUE)\rWordclouds\r\rWe’ve seen that this tidy text mining approach works well with ggplot2, but having our data in a tidy format is useful for other plots as well.\nFor example, consider the wordcloud package. Let’s look at the most common words in Charles Dickens’ two books as a whole again.\nlibrary(wordcloud)\rtidy_book %\u0026gt;%\rcount(word) %\u0026gt;%\rwith(wordcloud(word, n, max.words = 100))\rIn other functions, such as comparison.cloud, you may need to turn it into a matrix with reshape2’s acast. Let’s do the sentiment analysis to tag positive and negative words using an inner join, then find the most common positive and negative words. Until the step where we need to send the data to comparison.cloud, this can all be done with joins, piping, and dplyr because our data is in tidy format.\nlibrary(reshape2)\rtidy_book %\u0026gt;%\rinner_join(get_sentiments(\u0026quot;bing\u0026quot;)) %\u0026gt;%\rcount(word, sentiment, sort = TRUE) %\u0026gt;%\racast(word ~ sentiment, value.var = \u0026quot;n\u0026quot;, fill = 0) %\u0026gt;%\rcomparison.cloud(colors = c(\u0026quot;#F8766D\u0026quot;, \u0026quot;#00BFC4\u0026quot;),\rmax.words = 100)\rFor “Converting to and from Document-Term Matrix and Corpus objects”, You can visit here.\n","date":1563494400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563494400,"objectID":"fc55d8dca350c005528ed6012852a8dd","permalink":"https://yuan-du.com/post/tidy-text-mining/","publishdate":"2019-07-19T00:00:00Z","relpermalink":"/post/tidy-text-mining/","section":"post","summary":"Thanks to the short course at SDSS 2019, I learned how to do tf-idf to topic modeling and sentiment analysis by using tidytext taught by Julia Silge, author of Text Mining with R and Mara Averick. They did a great job on teaching the four hour class. I didn’t expect to have so much covered in the short course.\nHere is an example that I used the method to analyze A Tale of Two Cities and Great Expectations by Charles Dickens by using sentiment analysis.","tags":["Tidytext","Tidyverse"],"title":"Tidy Text Mining","type":"post"},{"authors":[],"categories":["Blogdown"],"content":"\r\r\r\rI’m a noobie on using Github and Hugo. 😊 It took me quite a while to figure out how to use version control on Github through Rstudio. In this blog, I would like to share my obstacles on publishing website to Github and deploying my website by Netflify. Some of the obstacles were caused by not understanding the structure and workflow of Github/Blogdown because I took a shortcut by googling and trials \u0026amp; errors and didn’t have patience to read all the details of the Yihui’s excellent guidelines. If you are a really new user as me with no website experience, hope this blog is helpful for you. You can skip some of the dump obstacles that you hopefully will not run into. 🙏\n\rMake sure your R and Rstudio version is up-to-date. Here you can find how to update R and Rstudio.\n\rSteps of creating blogdown in Rstudio, github repo and Netlify\n\r\rGeneral steps Recommended:\r\r{\"x\":{\"diagram\":\"digraph {\\ngraph [layout = dot, rankdir = LR]\\n\\n# define the global styles of the nodes. We can override these in box if we wish\\nnode [shape = rectangle, style = filled, fillcolor = Linen]\\nFirst [label = \\\"1. \\n Create \\n Github Repository\\\"]\\nSecond [label = \\\"2. \\n Create R blogdown \\n in R Studio\\\"]\\nThird [label= \\\"3. \\n Deploy the web \\n by Netlify\\\"]\\n\\n# edge definitions with the node IDs\\nFirst - Second - Third\\n}\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}\nI highly recommend you create a Github repository before you create blogdown as we have to use Github repo \u0026lt;repo name\u0026gt; to store all blogdown source files and deploy the _public_ folder by Netlify.Here are the steps that needs to pay attention when using Github:\r\r\rClick the Clone or download green button and copy URL.\rIn Rstudio terminal Change directory to Github repo you just created \u0026lt;repo name\u0026gt; by using cd \u0026lt;repo name\u0026gt;.\rRun git clone \u0026lt;GitHub URL\u0026gt; to clone new repository.\r\rBuild your blogdown in R Studio. I believe this step is easy to find either by watching the great RStudio Blogdown Webnair Video 2018 to get started or read some parts in the book by Yihui. You can refer to my previous blog about change to your own logo. You should decide if you will use R markdown or Markdown for each blog.\r\r\rA tip to select multiple items to commit is by git add .1 and git commit -m \"your message\" and git push origin master in the terminal.\rAdd a file for the post that contains pictures. To reference the picture from the post, a folder img should be created under the folder - static and you can reference it as for example, ![](/img/imgname.png).\rSometimes the Viewer doesn’t display the content. What worked for me is that I restart the R session and render my post with blogdown::serve_site() manually in the Console.Also, try to update packages by using update.packages(ask = FALSE, checkBuilt = TRUE)\rMy post that’s written by R markdown doesn’t generate html file and then the blog coudln’t be deployed. so by manually Knit to html to generate the file solved the problem.\rAdd emojis. In config.toml, put enableEmoji = true to enable the function. You can find emoji cheatsheet here.\r\rDeploy the website on netlify for free requires below steps:\r\r\rCreate a netlify account by linking with Github\rName your website in config.toml file under baseurl =. followed by the netlify subdomain .netlify.com/. Remember to have backslash “/” at the end.\rAdd the build settings. Push the “hugo” as the Build command and “public” as the Publish directory.\r#{idth=800px height=600px}\rcheck Hugo version blogdown::hugo_version()\r\rHope above solutions are helpful to you. 🍻\n\rThere is a space between “add” and “.”↩\n\r\r\r","date":1563408000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563408000,"objectID":"cdfdb26e78efe736b553bd35006cbf0a","permalink":"https://yuan-du.com/post/netlifypub/","publishdate":"2019-07-18T00:00:00Z","relpermalink":"/post/netlifypub/","section":"post","summary":"I’m a noobie on using Github and Hugo. 😊 It took me quite a while to figure out how to use version control on Github through Rstudio. In this blog, I would like to share my obstacles on publishing website to Github and deploying my website by Netflify. Some of the obstacles were caused by not understanding the structure and workflow of Github/Blogdown because I took a shortcut by googling and trials \u0026amp; errors and didn’t have patience to read all the details of the Yihui’s excellent guidelines.","tags":[],"title":"Publish R blogdown by Netlify","type":"post"},{"authors":null,"categories":["Blogdown"],"content":"I have no background of editing html and css. It took me a while to figure out how to modify the Hugo Lithium theme for my own blog. I started with this simple theme because the fancier theme, the more knowledge of configuration will be needed. This make it easier for me to transision to the Academic Thmeme that I\u0026rsquo;m currently using. so I would like to share it with you, by no means that this is the only way to do it.\nStep 1: Generate your own favicon ico by using free favicon generator. I generated it by using my inital of my first name.\nStep 2: Save the download package and save the favicon.ico logo in the path \\thmemes\\hugo-lithium\\static\\ (Hugo will copy it to root directory).and copy the small logo named apple-touch-icon.png under the path \\thmemes\\hugo-lithium\\static\\images\nStep 3: change the url logo.png name in the file config.toml under\n[params.logo] url = \u0026quot;apple-touch-icon.png\u0026quot;\n","date":1562457600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562457600,"objectID":"a54b107da98e1f4d7f58bc8808356425","permalink":"https://yuan-du.com/post/create-your-own-logo/","publishdate":"2019-07-07T00:00:00Z","relpermalink":"/post/create-your-own-logo/","section":"post","summary":"I have no background of editing html and css. It took me a while to figure out how to modify the Hugo Lithium theme for my own blog. I started with this simple theme because the fancier theme, the more knowledge of configuration will be needed. This make it easier for me to transision to the Academic Thmeme that I\u0026rsquo;m currently using. so I would like to share it with you, by no means that this is the only way to do it.","tags":["Logo"],"title":"How to create your own logo and apply to your own website","type":"post"},{"authors":[],"categories":["R Markdown"],"content":"\r\r\r\rR Markdown\rThe first time that I used R Markdown was back to 2014 when I was in graduate school. Unfortunately, I haven’t used this as much as I would like to due to the inconvinience with IT in the hospital. I’m slowly trying to use Rstudio more and more as SAS has limited options on some analysis. R markdown is great for documenting R code and math expressions.\nThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nYou can embed an R code chunk like this:\nsummary(cars)\r## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00\rfit \u0026lt;- lm(dist ~ speed, data = cars)\rfit\r## ## Call:\r## lm(formula = dist ~ speed, data = cars)\r## ## Coefficients:\r## (Intercept) speed ## -17.579 3.932\r\rIncluding Plots\rYou can also embed plots. See Figure 1 for example:\npar(mar = c(0, 1, 0, 1))\rpie(\rc(280, 60, 20),\rc(\u0026#39;Sky\u0026#39;, \u0026#39;Sunny side of pyramid\u0026#39;, \u0026#39;Shady side of pyramid\u0026#39;),\rcol = c(\u0026#39;#0292D8\u0026#39;, \u0026#39;#F7EA39\u0026#39;, \u0026#39;#C4B632\u0026#39;),\rinit.angle = -50, border = NA\r)\r\rFigure 1: A fancy pie chart.\r\r\rIncluding Flow Chart\rYou can embed a flow chart. See Figure 2 for example:\nDiagrammeR::grViz(\u0026quot;digraph {\rgraph [layout = dot, rankdir = LR]\r# define the global styles of the nodes. We can override these in box if we wish\rnode [shape = rectangle, style = filled, fillcolor = Linen]\rFirst [label = \u0026#39;1. \\n Create \\n Github Repository\u0026#39;]\rSecond [label = \u0026#39;2. \\n Create R blogdown \\n in R Studio\u0026#39;]\rThird [label= \u0026#39;3. \\n Deploy the web \\n by Netlify\u0026#39;]\r# edge definitions with the node IDs\rFirst -\u0026gt; Second -\u0026gt; Third\r}\u0026quot;, height=150)\r\r\r{\"x\":{\"diagram\":\"digraph {\\ngraph [layout = dot, rankdir = LR]\\n\\n# define the global styles of the nodes. We can override these in box if we wish\\n\\nnode [shape = rectangle, style = filled, fillcolor = Linen]\\nFirst [label = \\\"1. \\n Create \\n Github Repository\\\"]\\nSecond [label = \\\"2. \\n Create R blogdown \\n in R Studio\\\"]\\nThird [label= \\\"3. \\n Deploy the web \\n by Netlify\\\"]\\n\\n# edge definitions with the node IDs\\nFirst - Second - Third\\n}\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}\rFigure 2: A flow chart.\r\r\r","date":1562457600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568838545,"objectID":"fe5435aff8cee94d2b4312539f092fb5","permalink":"https://yuan-du.com/post/rmarkdown_intro/","publishdate":"2019-07-07T00:00:00Z","relpermalink":"/post/rmarkdown_intro/","section":"post","summary":"R Markdown\rThe first time that I used R Markdown was back to 2014 when I was in graduate school. Unfortunately, I haven’t used this as much as I would like to due to the inconvinience with IT in the hospital. I’m slowly trying to use Rstudio more and more as SAS has limited options on some analysis. R markdown is great for documenting R code and math expressions.","tags":["R Markdown"],"title":"R Markdown Introduction","type":"post"},{"authors":["Luwei Tao","Jingxin Sun","Tarek Mekhail","Lingbin Meng","Yuan Du","Mark A. Socinski","Amanda Allen","Brenda L. Rzeszutko","Chung-Che Chang"],"categories":null,"content":"","date":1558828800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558828800,"objectID":"6b709ac86773d0e9346edefc3774f20d","permalink":"https://yuan-du.com/publication/2019-5-jco/","publishdate":"2019-05-26T00:00:00Z","relpermalink":"/publication/2019-5-jco/","section":"publication","summary":"The clinical courses, including tumor stage at diagnosis, presence of brain metastasis, OS and RFS, are similar among lung adenocarcinoma patients with different KRAS mutation subtypes. Additionally, PD-L1 expression status appears to be independent of KRAS mutation subtypes. Of note, concurrent PD-L1 expression and G12C mutation is associated with particularly poorer prognosis. Further study is needed to see if PD1/PD-L1 block may improve outcome of this group of patients.","tags":["Lung Caner","Gene","Survival","Cancer","Oncology"],"title":"The prognostic value of KRAS mutation subtypes and PD-L1 expression in patients with lung adenocarcinoma","type":"publication"},{"authors":["J.A. Reza","C. Canavan","M. Uwah","K. Wissinger","P. Veldhuis","S. Patel","Y. Du","J.P. Arnoletti"],"categories":null,"content":"","date":1551398400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551398400,"objectID":"dac592ecb7457583b912f976a7b0d541","permalink":"https://yuan-du.com/publication/2019-3-hpb/","publishdate":"2019-03-01T00:00:00Z","relpermalink":"/publication/2019-3-hpb/","section":"publication","summary":"During PD, reconstruction over an external pancreatic stent led to very low PF incidence with no significant difference observed in severity between patients with PDAC and other high fistula risk pathologies. Routine external pancreatic stent placement after PD, regardless of PF risk, may reduce incidence of PF and need for additional interventions.","tags":["pancreatic fistula","pancreaticoduodenectomy","Hypothesis testing"],"title":"External pancreatic stents after pancreaticoduodenectomy reduce pancreatic fistula rates and severity","type":"publication"},{"authors":["G.D.Everett","L.Albadin","Yuan Du"],"categories":null,"content":"","date":1550102400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550102400,"objectID":"9304138c85289f3e425a9f609fc40c5f","permalink":"https://yuan-du.com/publication/2019-2-im/","publishdate":"2019-02-14T00:00:00Z","relpermalink":"/publication/2019-2-im/","section":"publication","summary":"The study sought to determine whether peer evaluations were favorably biased toward trainees of similar background.","tags":["Resident education","peer assessment","demographic bias","cultural bias"],"title":"The effect of demographic characteristics, Country of birth and country of medical training on the peer evaluations of internal medicine resident physicians","type":"publication"},{"authors":["Yi-Ling Lin","Yuan Du","Cristina Gomez","Judith Ortiz"],"categories":null,"content":"","date":1502064000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1502064000,"objectID":"9210040abdb17c8f7eefc46b2e90fb88","permalink":"https://yuan-du.com/publication/2017-8-pcmh/","publishdate":"2017-08-07T00:00:00Z","relpermalink":"/publication/2017-8-pcmh/","section":"publication","summary":"This study had a cross-sectional design using survey research. The unit of analysis was the RHC; the total sample size was 178.","tags":["ACO","Affordable Care Act","Survey","Population Health"],"title":"Does Patient-Centered Medical Home Recognition Relate to Accountable Care Organization Participation?","type":"publication"},{"authors":["Stephenie Poris","Andrew Fontaine","Julie Glener","Stacey Kubovec","Paula Veldhuis","Yuan Du","Julie Pepe","Steve Eubanks"],"categories":null,"content":"","date":1498089600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498089600,"objectID":"cb9f6d7c408632360868c3886ae1a4a6","permalink":"https://yuan-du.com/publication/2017-6-gastric/","publishdate":"2017-06-22T00:00:00Z","relpermalink":"/publication/2017-6-gastric/","section":"publication","summary":"There are currently no evidence-based guidelines on the routine or selective use of radiologic imaging after omental patch repair (OPR) of a gastric (GP) or duodenal perforation (DP). This study aims to elucidate whether the use of selective or routine contrast upper gastrointestinal series (UGI) postoperatively will significantly increase the rate of missed leaks or lead to worse clinical outcomes.","tags":["Contrast upper gastrointestinal series","UGI","Omental patch","Gastric perforation","Duodenal perforation","Graham patch","Surgery"],"title":"Routine versus selective upper gastrointestinal contrast series after omental patch repair for gastric or duodenal perforation","type":"publication"},{"authors":["Khalid Abusaada MD FACP","Leen Alsaleh MD","Victor Herrera MD","Yuan Du","Hassan Baig MD","George Everett MD FACP"],"categories":null,"content":"","date":1483488000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483488000,"objectID":"121b2206f5876804c340973600c0b4dd","permalink":"https://yuan-du.com/publication/2017-1-copd/","publishdate":"2017-01-04T00:00:00Z","relpermalink":"/publication/2017-1-copd/","section":"publication","summary":"A retrospective cohort study of patients admitted for a primary diagnosis of chronic obstructive pulmonary disease exacerbation to Florida Hospital Orlando, a large community teaching hospital, between January 1, 2011, and December 31, 2014. Data were extracted from Premier administrative database. Risk adjusted length of stay (LOS), cost of hospitalization, 30‐day readmissions, and mortality rate were measured. Risk adjustment for outcomes was based on Premier CareScience methodology.","tags":["chronic obstructive pulmonary disease","Cost","Clinical outcomes","LOS","resource use","Risk adjustment"],"title":"Comparison of hospital outcomes and resource utilization in acute COPD exacerbation patients managed by teaching versus non-teaching services in a community hospital","type":"publication"},{"authors":["Wan, T.T.H.","Judith Ortiz","Yuan Du"],"categories":null,"content":"","date":1442275200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1442275200,"objectID":"ca65656da8b1aeee1c0db82a0439e147","permalink":"https://yuan-du.com/publication/2015-9-rhc/","publishdate":"2015-09-15T00:00:00Z","relpermalink":"/publication/2015-9-rhc/","section":"publication","summary":"A generalized estimating equation of sixteen predictors was analyzed for the variability in risk-adjusted readmission rates.","tags":["Rural health clinics","Affordable Care Act","ACO","readmissions Risk-adjusted rate","Generalized estimating equation"],"title":"Contextual, Organizational and Ecological Effects on the Variations in Hospital Readmissions of Rural Medicare Beneficiaries in Eight Southeastern States","type":"publication"}]