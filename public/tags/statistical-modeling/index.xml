<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Statistical Modeling | Yuan Du</title>
    <link>https://yuan-du.com/tags/statistical-modeling/</link>
      <atom:link href="https://yuan-du.com/tags/statistical-modeling/index.xml" rel="self" type="application/rss+xml" />
    <description>Statistical Modeling</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© Yuan Du, 2021 </copyright><lastBuildDate>Thu, 30 Jul 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://yuan-du.com/img/icon-192.png</url>
      <title>Statistical Modeling</title>
      <link>https://yuan-du.com/tags/statistical-modeling/</link>
    </image>
    
    <item>
      <title>Research Experience - Comparing Statistics vs Machine Learning</title>
      <link>https://yuan-du.com/post/2020-07-30-statvsml/statisticsvsml/</link>
      <pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://yuan-du.com/post/2020-07-30-statvsml/statisticsvsml/</guid>
      <description>
&lt;link href=&#34;https://yuan-du.com/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://yuan-du.com/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;There are so many terms regarding the field of Statistics and Data Science. We often heard Statistics, Data Mining, Machine Learning, Big Data, etc. It especially confuses people that’s in a different field. I remember that over five years ago, a radiologist asked me if I can mine data from the radiology system because she saw that I have Data Mining skills. I was blown away by the understanding of Data Mining to a doctor. Data Mining and data extraction is totally different. After data extraction and data preparation, data mining is used to identify patterns and relationships based on the research/business questions.&lt;/p&gt;
&lt;p&gt;Generally speaking, due to the storage and advancement of computers, our data analysis power which builds on Statistical knowledge expanded by using more complicated statistical theory and algorithms that are applied to multidisciplinary science such as Biostatistics, Medicine, Public Health, Computer Science, Engineering, Physicis, etc.
&lt;img src=&#34;https://yuan-du.com/img/History-DM.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Nature has a paper &lt;a href=&#34;https://www.nature.com/articles/nmeth.4642#:~:text=Statistics%20draws%20population%20inferences%20from,learning%20finds%20generalizable%20predictive%20patterns.&amp;amp;text=Two%20major%20goals%20in%20the,systems%20are%20inference%20and%20prediction.&#34;&gt;“Statistics versus machine learning”&lt;/a&gt; that explains the relationships.&lt;a href=&#34;https://www.aaai.org/ojs/index.php/aimagazine/article/view/1230/1131&#34;&gt;From Data Mining to Knowledge Discovery in Databases&lt;/a&gt; discussed and summrized the history of Knowledge discovery of database (KDD).&lt;/p&gt;
&lt;p&gt;In the realm of healthcare research studies, I would like to share my own experience of what types of statistical learning were used. Based on the objectives of a study, we generally have two types of goals:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Inference&lt;/strong&gt;: Identify risk factors that associate with response outcome(s). It normally has smaller sample size. This is the most common goal in medical research. It requires clinical knowledge to start with research questions that involve hypothesis. Univariate analysis (Hypothesis testing) and Multivariable analysis are used. Both types of analysis need assumptions on the data distribution, variance and linear/nonlinear relationship with response variable(s) to perform correct statistical tests. For univariate analysis, please check out my slides for the most commonly used &lt;a href=&#34;https://yuan-du.com/slides/2019-09-advancedstat&#34;&gt;hypothesis testings&lt;/a&gt;. The most common problem is &lt;code&gt;significance (p-value) fishing&lt;/code&gt;. There are difference p-value adjustment methods to consider when there are multiple testings. Physicians/researchers often want to publish significant testing result only which is not healthy for medical research. Non significant factors are important to the literature. It’s useful for meta analysis. For multivariable analysis, here are some examples that difference statistical models were used:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Clinical Outcome Study&lt;/strong&gt;: &lt;a href=&#34;https://yuan-du.com/publication/2017-1-copd/&#34;&gt;Comparison of hospital outcomes and resource utilization in acute COPD exacerbation patients managed by teaching versus non-teaching services in a community hospital&lt;/a&gt;: The data was from both national database &lt;a href=&#34;https://www.premierinc.com/about&#34;&gt;Premier&lt;/a&gt; and hospital EHR(Electric Health Record). &lt;code&gt;Multiple logistic regression&lt;/code&gt; was used for the multivariable analysis to identify the factors that contribute to resource utilization in the acute COPD patients.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Commercial Device Study&lt;/strong&gt;: &lt;a href=&#34;https://yuan-du.com/publication/2020-4-pharm/&#34;&gt;Characterisation of ICU sleep by a commercially available activity tracker and its agreement with patient-perceived sleep quality&lt;/a&gt;: This data was collected from ICU patients that used fitbits as alternative sleep tracking devices. Since each patient was measured several times, a &lt;code&gt;mixed model repeated measure&lt;/code&gt; was used to detect the correlation/agreement between each sleep quality measure and the gold standard Richard-Campbell Sleep Questionnaire (RCSQ). Instead of a single pearson correlation coefficient, the &lt;code&gt;bootstrap method&lt;/code&gt; with 1000 times was implemented to generate Confidence Interval for statistical inference.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Population Health Study&lt;/strong&gt;: &lt;a href=&#34;https://yuan-du.com/publication/2015-9-rhc/&#34;&gt;Contextual, Organizational and Ecological Effects on the Variations in Hospital Readmissions of Rural Medicare Beneficiaries in Eight Southeastern States&lt;/a&gt;: This a longitudinal study funded by NIH. Data was from mainly from CMS data warehouse on beneficiaries and providers. First, &lt;code&gt;risk-adjusted readmission&lt;/code&gt; was calculated by Logistic regression model on patient level. Then Generalized Estimating Equation (&lt;code&gt;GEE&lt;/code&gt;) method was performed on the rurual clinic level for 6 years of data. This is a type of &lt;code&gt;hierarchical regression&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If the number of variables is very large compared to observations (p&amp;gt;n), for example genomics, a person has hundreds of genes. Or when the ratio of p/n is larger than normal and the linear/nonlinear relationships and assumptions are vague, noval machine learning methods are preferred.&lt;/p&gt;
&lt;p&gt;One example is the &lt;a href=&#34;https://yuan-du.com/talk/2019-11-25-breast-cancer-by-svm/&#34;&gt;breast cancer tumor classification&lt;/a&gt;. Another example is a Leukemia project that i’m currently working on to identify unknown gene mutation effects to the mortality of the patients. There are only 125 patients, and each patient has over 38 gene mutations. The gene mutations are sparse. Methods with penalty and constraints will be suitable for this type of data. I’ll discuss more about this project seperately later.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Prediction&lt;/strong&gt;: Predict outcomes. It preferrs big sample size for better prediction accuracy.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Covid 19 Study&lt;/strong&gt; &lt;a href=&#34;https://pubmed.ncbi.nlm.nih.gov/27544541/&#34;&gt;This paper&lt;/a&gt; was reference for prediciton: Due to the extensive research studies on Covid 19. Our hospital identified various data and interesting risk factors to predict Covid 19 positive cases. On one hand, the study aims to identify additional risk factors. and on the other hand, with over 10K patients’ data, the study aims to predict Covid 19 cases based on the massive data. &lt;code&gt;Multiple logistic regression&lt;/code&gt;, &lt;code&gt;Random Forest&lt;/code&gt;, and &lt;code&gt;XGboost&lt;/code&gt; were used to predict the outcome. Since the risk factors and response variable have more linear relationship, and with a better interpretability, &lt;code&gt;Multiple logistic regression&lt;/code&gt; with training and validation test was picked and each patient has a risk score for decision makers to utilize the hospital resources.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Closing Note&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In healthcare research, asking the right questions and have clinical knowledge is very essential to determine the patient population and appropriate methods. Understanding the problems and using the efficient methods provides a strong solution. Statistical inference is essential in traditional Health care research. Maching learning method is more flexible and is generally better for prediction, big data or unknown assumptions.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian Analysis</title>
      <link>https://yuan-du.com/post/2020-04-18-bayesian-analysis/bayesian-analysis/</link>
      <pubDate>Sat, 18 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://yuan-du.com/post/2020-04-18-bayesian-analysis/bayesian-analysis/</guid>
      <description>


&lt;p&gt;Bayesian approach becomes more and more popular because of the improvement of the mordern computing ability for machine learning and big data. Bayesian analysis is a completely different approach compared to frequentist approach. Yet, it’s more challenging. To be able to understand and learn the Bayesian approach, &lt;code&gt;first&lt;/code&gt;, we will need to have the knowledge of conditional probability. &lt;code&gt;Second&lt;/code&gt;, to have the knowledge of different distributions such as normal, bernoulli, binomial, gamma, beta, cauchy, possion, etc. &lt;code&gt;Third&lt;/code&gt;, to be familiar with calculus. We need to calculate derivatives and integration of different distributions. &lt;code&gt;Last but not at least&lt;/code&gt;, to be familiar with simulation sampling techniques such as Grid sampling, Variational Bayes and Monte Carlo Markov Chian (MCMC) including popular Gibbs sampling, Metropolis Hastings Sampling, Hamiltonian Monte Carlo (HMC),etc. Luckily, there are a few software tools &lt;a href=&#34;https://web.sgh.waw.pl/~atoroj/ekonometria_bayesowska/jags_user_manual.pdf&#34;&gt;Jags&lt;/a&gt;, &lt;a href=&#34;https://mc-stan.org/users/documentation/&#34;&gt;Stan&lt;/a&gt; .etc can be used for Gibbs sampling and HMC.
&lt;img src=&#34;https://yuan-du.com/img/Human.jpg&#34; alt=&#34;‘Credit: https://www2.isye.gatech.edu/~brani/isyebayes/jokes.html’&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The concept is simple. According to Bayes Theorem &lt;span class=&#34;math inline&#34;&gt;\(p(\theta|y) = \frac{p(y|\theta)p(\theta)}{p(y)}\)&lt;/span&gt; or without normalization &lt;span class=&#34;math inline&#34;&gt;\(p(\theta|y) \sim p(y|\theta)p(\theta)\)&lt;/span&gt;, we want to gain the posterior distribution by using prior information and likelihood of the data information. Most of the time, we use log likelihood instead for easier calculation since &lt;span class=&#34;math inline&#34;&gt;\(log( a*b )= loga +logb\)&lt;/span&gt;. Bayesian approach involves much more math than frequentist approach and more complicated. Frequentist focus on one point estimate (hypothesis and Confidence Interval), and bayesian focuses on a looking forward range (Credible Interval).
&lt;img src=&#34;https://yuan-du.com/img/FreBay.png&#34; alt=&#34;Credit: https://365datascience.com/bayesian-vs-frequentist-approach/&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If friquentist and bayesian approach are just two different ways to look into things, Why and when is recommended to use Bayesian approach instead of frequentist approach?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Here are a few examples to use Bayesian approach over frequentist approach:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Clear prior information: the example in the book &lt;a href=&#34;http://www.stat.columbia.edu/~gelman/book/BDA3.pdf&#34;&gt;BD3&lt;/a&gt; of Andrew Gelman’s in Chapter 1, problem 6. The prior information is that approximately 1/125 of all births are fraternal twins and 1/300 of births are identical twins.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Seperation problems. For example logistic regression couldn’t converge due to high dimension and small sample size.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Estimate multiple outcomes with credible interval. For example, family doctors try to diagnosis diseases (such as cold, flu) based on multiple symptoms (such as headache, sore throat, high temprature) and the probabilities of all symptoms sum up to limited possible diseases.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Small sample size with multiple experiments and limited budget.It’s a preferred meta analysis than tranditional meta analysis that has high heterogeneity with different recources since it provides a credible interval instead of confident interval.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;Reference:&lt;/code&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://www.stat.columbia.edu/~gelman/stuff_for_blog/ohagan.pdf&#34;&gt;Dicing with the unknown&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.fda.gov/regulatory-information/search-fda-guidance-documents/guidance-use-bayesian-statistics-medical-device-clinical-trials#3&#34;&gt;FDA Guidance for the Use of Bayesian Statistics in Medical Device Clinical Trials&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>SIR Model for Covid19</title>
      <link>https://yuan-du.com/post/2020-04-17-sir-model/2020-04-18-sir-model-for-covid19/</link>
      <pubDate>Sat, 18 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://yuan-du.com/post/2020-04-17-sir-model/2020-04-18-sir-model-for-covid19/</guid>
      <description>


&lt;p&gt;It’s been a hectic few months since my last post. Working and schooling at home during this pandemic chaos is challenging, but it doesn’t stop the hope of renew and transformation. One of the most popular statistcal modeling method for epidemiology and network science-Spreading Phenomena is SIR (susceptible-infected-recovered) model from &lt;a href=&#34;https://timchurches.github.io/blog/posts/2020-02-18-analysing-covid-19-2019-ncov-outbreak-data-with-r-part-1/&#34;&gt;Tim Churches&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Susceptible (S)&lt;/strong&gt;: Healthy individuals who have not yet contacted the pathogen.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Infectious (I)&lt;/strong&gt;: Contagious individuals who have contacted the pathogen and hence can infect others.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recovered/Removed (R)&lt;/strong&gt;: Infected individuals become immune or die, i.e., will not be infected again and cannot infect anyone else.
&lt;img src=&#34;https://yuan-du.com/img/SIR-SIRS.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As shown in the picture above, there are two important rates &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;(transmission rate of the pathogen), &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt;(recovery rate). You probablily heard of the basic reproduction number &lt;span class=&#34;math inline&#34;&gt;\(R_0\)&lt;/span&gt; value (R-nought), which can be calculated by &lt;span class=&#34;math inline&#34;&gt;\(\frac{\beta}{\gamma}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(R_0\)&lt;/span&gt; &amp;gt; 1, epidemic is in the epidemic state;&lt;/p&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(R_0\)&lt;/span&gt; &amp;lt; 1, epidemic dies out.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The purpose of the post is to estiamte the basic reproduction number R-nought / &lt;a href=&#34;https://en.wikipedia.org/wiki/Basic_reproduction_number&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(R_0\)&lt;/span&gt;&lt;/a&gt; value based on US data and then make predictions for the future.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. Load packages&lt;/strong&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#install.packages(c(&amp;quot;httr&amp;quot;, &amp;quot;jsonlite&amp;quot;))
library(httr)
library(jsonlite)
#install.packages(&amp;quot;optimr&amp;quot;)
library(optimr)
library(deSolve)
library(tidyverse)
library(lubridate)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;2. Retrieve data from John’s Hopkins and use date March 1st till March 15 to fit SIR Model and check if the fit is reasonable&lt;/strong&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#by Johns Hopkins CSSE
res = GET(&amp;quot;https://pomber.github.io/covid19/timeseries.json&amp;quot;)

data = fromJSON(rawToChar(res$content))
#names(data) all contries 
US = data$US
US$date=as.Date(US$date) 

new&amp;lt;-US %&amp;gt;% dplyr::filter(date&amp;gt;= &amp;quot;2020-03-01&amp;quot;&amp;amp; date&amp;lt;&amp;quot;2020-03-15&amp;quot;) #for US data only
head(new) #first six rows&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         date confirmed deaths recovered
## 1 2020-03-01        32      1         7
## 2 2020-03-02        54      6         7
## 3 2020-03-03        74      7         7
## 4 2020-03-04       107     11         7
## 5 2020-03-05       184     12         7
## 6 2020-03-06       237     14         7&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tail(new) #last six rows&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          date confirmed deaths recovered
## 9  2020-03-09       589     22         7
## 10 2020-03-10       782     28         8
## 11 2020-03-11      1145     33         8
## 12 2020-03-12      1584     43        12
## 13 2020-03-13      2214     51        12
## 14 2020-03-14      2971     58        12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;3. build SIR Model and find &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; by optimization method &lt;a href=&#34;https://en.wikipedia.org/wiki/Residual_sum_of_squares&#34;&gt;RSS&lt;/a&gt;&lt;/strong&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Initial values:
Infected &amp;lt;- new$confirmed
Day &amp;lt;- 1:(length(new$date))
N&amp;lt;-327200000 #US population
init &amp;lt;- c(S = N - Infected[1], I = Infected[1], R = 0) #Inital value

## SIR Model
SIR &amp;lt;- function(time, state, parameters) {
  par &amp;lt;- as.list(c(state, parameters))
  with(par, {
    dS &amp;lt;- -beta * I * S/N
    dI &amp;lt;- beta * I * S/N - gamma * I
    dR &amp;lt;- gamma * I
    list(c(dS, dI, dR))
  })
}

#Optimization by RSS to get beta and gamma
RSS &amp;lt;- function(parameters) {
  names(parameters) &amp;lt;- c(&amp;quot;beta&amp;quot;, &amp;quot;gamma&amp;quot;)
  out &amp;lt;- ode(y = init, times = Day, func = SIR, parms = parameters)
  fit &amp;lt;- out[, 3]
  sum((Infected - fit)^2)
}

Opt &amp;lt;- optim(c(0.5, 0.5), RSS, method = &amp;quot;L-BFGS-B&amp;quot;, lower = c(0,0), upper = c(1, 1))

Opt_par &amp;lt;- setNames(Opt$par, c(&amp;quot;beta&amp;quot;, &amp;quot;gamma&amp;quot;))
Opt_par&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      beta     gamma 
## 0.6755947 0.3244053&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#R0
Opt$par[1]/Opt$par[2]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.082563&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;4. Plot the predicted value vs raw data&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sir_start_date &amp;lt;- &amp;quot;2020-03-01&amp;quot;
t &amp;lt;- 1:as.integer(ymd(&amp;quot;2020-03-15&amp;quot;) - ymd(sir_start_date))

# get the fitted values from our SIR model
fitted_cumulative_incidence &amp;lt;- data.frame(ode(y = init, times = t, 
                                              func = SIR, parms = Opt_par))
# add a Date column and join the observed incidence data
fitted_cumulative_incidence &amp;lt;- fitted_cumulative_incidence %&amp;gt;% 
  mutate(date = ymd(sir_start_date) + days(t - 1)) %&amp;gt;% 
  left_join(new %&amp;gt;% ungroup() %&amp;gt;% select(date, confirmed))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Joining, by = &amp;quot;date&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plot the data
fitted_cumulative_incidence %&amp;gt;% 
  ggplot(aes(x = date)) + geom_line(aes(y = I), colour = &amp;quot;red&amp;quot;) + 
  geom_point(aes(y = confirmed), colour = &amp;quot;orange&amp;quot;) + 
  labs(y = &amp;quot;Cumulative incidence&amp;quot;, title = &amp;quot;COVID-19 fitted vs observed cumulative incidence, US 03/01-03/15&amp;quot;, 
       subtitle = &amp;quot;(red=fitted incidence from SIR model, orange=observed incidence)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yuan-du.com/post/2020-04-17-SIR-model/2020-04-18-sir-model-for-covid19_files/figure-html/plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;5. The fit looks reasonable with R0=1.78, now we use the model to predict the curve for 3 months starting from March&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Prediction
# time in days for predictions
t &amp;lt;- 1:90
# get the fitted values from our SIR model
fitted_cumulative_incidence &amp;lt;- data.frame(ode(y = init, times = t, 
                                              func = SIR, parms = Opt_par))
# add a Date column and join the observed incidence data
fitted_cumulative_incidence &amp;lt;- fitted_cumulative_incidence %&amp;gt;% 
  mutate(date = ymd(sir_start_date) + days(t - 1)) %&amp;gt;% 
  left_join(new %&amp;gt;% ungroup() %&amp;gt;% select(date, confirmed))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Joining, by = &amp;quot;date&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plot the data
fitted_cumulative_incidence %&amp;gt;% ggplot(aes(x = date)) + geom_line(aes(y = I), 
                                                                  colour = &amp;quot;red&amp;quot;) + geom_line(aes(y = S), colour = &amp;quot;black&amp;quot;) + 
  geom_line(aes(y = R), colour = &amp;quot;green&amp;quot;) + geom_point(aes(y = confirmed), 
                                                       colour = &amp;quot;orange&amp;quot;) + scale_y_continuous(labels = scales::comma) + 
  labs(y = &amp;quot;Persons&amp;quot;, title = &amp;quot;COVID-19 3 months prediction&amp;quot;) + 
  scale_colour_manual(name = &amp;quot;&amp;quot;, values = c(red = &amp;quot;red&amp;quot;, black = &amp;quot;black&amp;quot;, 
                                            green = &amp;quot;green&amp;quot;, orange = &amp;quot;orange&amp;quot;), labels = c(&amp;quot;Susceptible&amp;quot;, 
                                                                                            &amp;quot;Recovered&amp;quot;, &amp;quot;Observed incidence&amp;quot;, &amp;quot;Infectious&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yuan-du.com/post/2020-04-17-SIR-model/2020-04-18-sir-model-for-covid19_files/figure-html/Prediction-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusion: It looks like the peak will be in the end of April and it will die down at the end of May.&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Breast Cancer Image Classification Analysis by SVM</title>
      <link>https://yuan-du.com/talk/2019-11-25-breast-cancer-by-svm/</link>
      <pubDate>Wed, 25 Dec 2019 16:30:00 +0000</pubDate>
      <guid>https://yuan-du.com/talk/2019-11-25-breast-cancer-by-svm/</guid>
      <description>&lt;p&gt;Breast Cancer Image Classification Analysis Mainly by SVM&lt;/p&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click on the &lt;strong&gt;report&lt;/strong&gt; button above to view the built-in slides feature.
  &lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Awesome data science presentations and tools</title>
      <link>https://yuan-du.com/post/tools-reference/</link>
      <pubDate>Fri, 16 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://yuan-du.com/post/tools-reference/</guid>
      <description>&lt;p&gt;Recently, I discovered a few pretty cool tools, R-based and easy to follow.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;R Presentation Themes&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Why uses R presentation?&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Take advantage of &lt;a href=&#34;https://yuandueldaif.netlify.com/post/rmarkdown_intro/&#34; target=&#34;_blank&#34;&gt;R Markdown&lt;/a&gt; .&lt;/strong&gt; You can write all your slides in Markdown text&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Power of interchange.&lt;/strong&gt; You can include chunks of R code and rendered output like plots, results, tables, etc. in your slides&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Version control and sharing.&lt;/strong&gt; You can use git for version control and share your GitHub repository&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;My current favorate &lt;a href=&#34;https://github.com/yihui/xaringan/wiki/Themes&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Xaringan&lt;/strong&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/emitanaka/ninja-theme&#34; target=&#34;_blank&#34;&gt;nanja-theme&lt;/a&gt; was developed by Emi Tanaka.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Data Cleaning/Data Wrangling tool kit - Tidyverse&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Book&lt;/strong&gt; &lt;a href=&#34;https://r4ds.had.co.nz/&#34; target=&#34;_blank&#34;&gt;R for Data Science&lt;/a&gt; by Garrett Grolemund &amp;amp; Hadley Wickham.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Visual&lt;/strong&gt; Data cleaning post- &lt;a href=&#34;https://jules32.github.io/2016-07-12-Oxford/dplyr_tidyr/#3_tidyr_overview&#34; target=&#34;_blank&#34;&gt;Tidyverse overview&lt;/a&gt; by Julia Stewart Lowndes.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;I&amp;rsquo;m finishing up on an advanced statistical class &lt;a href=&#34;https://yuandueldaif.netlify.com/talk/2019-09-advancedstat/&#34; target=&#34;_blank&#34;&gt;&amp;ldquo;Significance Test&amp;rdquo;&lt;/a&gt; by using R presentation and will share the code on Github.
&lt;br/&gt;so stay tuned&amp;hellip;&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
