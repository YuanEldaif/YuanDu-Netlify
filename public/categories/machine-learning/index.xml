<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning | Yuan Du</title>
    <link>https://yuan-du.com/categories/machine-learning/</link>
      <atom:link href="https://yuan-du.com/categories/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Machine Learning</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© Yuan Du, 2021 </copyright><lastBuildDate>Mon, 07 Jun 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://yuan-du.com/img/icon-192.png</url>
      <title>Machine Learning</title>
      <link>https://yuan-du.com/categories/machine-learning/</link>
    </image>
    
    <item>
      <title>Learning To Rank (LTR)</title>
      <link>https://yuan-du.com/post/2021-06-07-ltr/decision-theory/</link>
      <pubDate>Mon, 07 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://yuan-du.com/post/2021-06-07-ltr/decision-theory/</guid>
      <description>


&lt;p&gt;Previously, in the post &lt;a href=&#34;https://yuan-du.com/post/2020-12-13-loss-functions/decision-theory/&#34;&gt;Loss Functions in Machine Learning and LTR&lt;/a&gt; we disscussed about how loss functions were used in ML and briefly mentioned LTR. Here I’ll discuss about LTR. LTR uses Machine Learning (ML)/Artifical Intelligence (AI) to predict rankings/ordinal data. It’s useful for google search, drug discovery, bioinformatics. Here is a list that seperates traditional ML from LTR:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Solve a ranking on a list of items&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Predict the optimal ordering of the list&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Doesn’t care much about the score of each item/point&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;only care the relative score/ordering among all the items&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For example, if we have 2 ML models to predict students’ score. and our goal is to rank students. and we have below results from the ML models. In this case, &lt;span style=&#34;color:red&#34;&gt;Model 2&lt;/span&gt; is better at ranking compared to Model 1 even though Model 1 has better prediction accuracy. Rank error is pair-wise based and is defined as &lt;span class=&#34;math inline&#34;&gt;\(\frac{ \# \textrm{ of discordant pairs} }{ \#\textrm{ of total pairs between + and -} }\)&lt;/span&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Student&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;True Score&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Model 1&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Model 2&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Student1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;90%&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;88%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;100%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Student2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;85%&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;89%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;50%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Student3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;80%&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;83%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;10%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;LTR system includes bipartite ranking, k-partite ranking, real value based ranking. We only talk about bipartite ranking here.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. &lt;span style=&#34;color:green&#34;&gt;Bipartite RankSVM Algorithm&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Bipartite RankSVM Algorithm uses hinge loss. The hinge loss is a loss function used for “maximum-margin” classification, most notably for support vector machine (SVM). It’s equivalent to minimize the loss function &lt;span class=&#34;math inline&#34;&gt;\(L_{hinge}(f,x_i^+,x_i^-) = [1-(f(x_i^+)-f(x_i^-))]_+ [u_+ = max(u,0)]\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;With &lt;span class=&#34;math inline&#34;&gt;\(f = W * X =\)&lt;/span&gt; ranking score, the optimization problem is loss + penalty:
&lt;span class=&#34;math display&#34;&gt;\[ \min_{f \in F_k} \frac{1}{mn}\sum_{i=1}^{m} \sum_{j=1}^{n}L_{hinge}(f,x_i^+,x_i^-) + \frac{\lambda}{2}||f||_k^2 \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Thus, the term &lt;span class=&#34;math inline&#34;&gt;\(f(x_i^+)-f(x_i^-)\)&lt;/span&gt; the larger, the better.If &lt;span class=&#34;math inline&#34;&gt;\(f(x_i^+)-f(x_i^-) &amp;lt;0\)&lt;/span&gt;, it means that it’s making mistakes so the objection function is penalized.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. &lt;span style=&#34;color:DeepSkyBlue&#34;&gt;Bipartite RankBoost Algorithm&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Bipartite RankBoost Algorithm uses the exponential loss.&lt;/p&gt;
&lt;p&gt;The population minimizer is:
&lt;span class=&#34;math display&#34;&gt;\[\min_{f \in L(F_{base})} \frac{1}{mn}\sum_{i=1}^{m} \sum_{j=1}^{n}L_{exp}(f,x_i^+,x_i^-)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(L_{exp}(f,x_i^+,x_i^-) = exp(-f(x_i^+)-f(x_i^-))\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3. &lt;span style=&#34;color:Gold&#34;&gt;Bipartite RankNet Algorithm&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Bipartite RankNet Algorithm uses the logistic loss (binomial log-likelihood loss or cross entropy loss).&lt;/p&gt;
&lt;p&gt;The binomial log-likelihood loss function is:
&lt;span class=&#34;math display&#34;&gt;\[\min_{f \in F_{neural}} \frac{1}{mn}\sum_{i=1}^{m} \sum_{j=1}^{n}L_{logistic}(f,x_i^+,x_i^-)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(L_{logistic}(f,x_i^+,x_i^-) = log(1+ exp((-f(x_i^+)-f(x_i^-)))\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;
&lt;em&gt;Reference:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://archive.siam.org/meetings/sdm10/tutorial1.pdf&#34;&gt;Computer Science &amp;amp; Artificial Intelligence Laboratory, MIT&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Loss Functions in Machine Learning and LTR</title>
      <link>https://yuan-du.com/post/2020-12-13-loss-functions/decision-theory/</link>
      <pubDate>Sun, 13 Dec 2020 22:27:29 -0400</pubDate>
      <guid>https://yuan-du.com/post/2020-12-13-loss-functions/decision-theory/</guid>
      <description>
&lt;link href=&#34;https://yuan-du.com/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://yuan-du.com/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Previously, &lt;a href=&#34;https://yuan-du.com/post/2020-09-23-decision-theory/decision-theory/&#34;&gt;decision theory&lt;/a&gt; was disscussed and an important part of is to evaluate a decision rule for decision making. Since the risk is the average &lt;strong&gt;loss &lt;span class=&#34;math inline&#34;&gt;\(L(\theta,d)\)&lt;/span&gt;&lt;/strong&gt;, different loss functions were used in Machine Learning models. Mean squared error (MSE) was mentioned as the most famous meausre by using squared error loss proposed by Gauss. Regression, Linear discriminant analysis (&lt;a href=&#34;https://en.wikipedia.org/wiki/Linear_discriminant_analysis&#34;&gt;LDA&lt;/a&gt;) use squared error loss. The squared loss function tends to penalize outliers excessively, leading to slower convergence rates. There are other popular loss functions and they are applied in various Machine Learning and Deep Learning models. The plot shows the loss function for two class classification.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://yuan-du.com/img/Hinge_expo_log.jpg&#34; alt=&#34;Loss Functions: &amp;quot;Loss vs y*f(x); y= \pm 1, the prediction is f, with class prediction sign(f)&amp;quot;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Loss Functions: &#34;Loss vs y*f(x); &lt;span class=&#34;math inline&#34;&gt;\(y= \pm 1\)&lt;/span&gt;, the prediction is f, with class prediction sign(f)&#34;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;1. &lt;span style=&#34;color:green&#34;&gt;Hinge loss&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The hinge loss is a loss function used for “maximum-margin” classification, most notably for support vector machine (SVM).It’s equivalent to minimize the loss function &lt;span class=&#34;math inline&#34;&gt;\(L(y,f) = [1-yf]_+\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;With &lt;span class=&#34;math inline&#34;&gt;\(f(x) = h(x)^T \beta + \beta_0\)&lt;/span&gt;, the optimization problem is loss + penalty:
&lt;span class=&#34;math display&#34;&gt;\[ \min_{\beta_0,\beta} \sum_{n=1}^{\infty}[1-y_if(x_i)]_+ + \frac{\lambda}{2}||\beta||^2 \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. &lt;span style=&#34;color:DeepSkyBlue&#34;&gt;Exponential loss&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The exponential loss is convex and grows exponentially for negative values which makes it more sensitive to outliers. The exponential loss is used in the &lt;a href=&#34;https://en.wikipedia.org/wiki/AdaBoost&#34;&gt;AdaBoost algorithm&lt;/a&gt;. The principal attraction of exponential loss in the context of additive modeling is computational. The additive expansion produced by AdaBoost is estimating onehalf of the log-odds of P(Y = 1|x). This justifies using its sign as the classification rule.&lt;/p&gt;
&lt;p&gt;The population minimizer is:
&lt;span class=&#34;math display&#34;&gt;\[f^*(x) = \arg\min_{f(x)} E_{Y|x}(e^{-Yf(x)}) = \frac{1}{2} log\frac{Pr(Y = 1|x)}{Pr(Y = -1|x)}\]&lt;/span&gt;
or
&lt;span class=&#34;math display&#34;&gt;\[Pr(Y = 1|x) = \frac{1}{1+e^{-2f*(x)}}\]&lt;/span&gt;
&lt;strong&gt;3. &lt;span style=&#34;color:Gold&#34;&gt;Logistic loss(Binomial Deviance)&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The logistic loss is also called as binomial log-likelihood loss or cross entropy loss. It’s used for logistic regression and in the &lt;a href=&#34;https://en.wikipedia.org/wiki/LogitBoost&#34;&gt;LogitBoost algorithm&lt;/a&gt;. The cross entropy loss is ubiquitous in &lt;a href=&#34;https://en.wikipedia.org/wiki/Deep_learning&#34;&gt;deep neural networks/Deep Learning&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The binomial log-likelihood loss function is:
&lt;span class=&#34;math display&#34;&gt;\[l(Y,p(x)) = Y&amp;#39;logp(x) + (1-Y&amp;#39;)log(1-p(x))\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;or the deviance
&lt;span class=&#34;math display&#34;&gt;\[-l(Y,f(x) = log(1+e^{-2Yf(x)})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Summary Table&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;11%&#34; /&gt;
&lt;col width=&#34;28%&#34; /&gt;
&lt;col width=&#34;37%&#34; /&gt;
&lt;col width=&#34;22%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Loss Function&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(L[y,f(x)]\)&lt;/span&gt;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Minimizing Function&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Advantage/Disadvantage&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Squared loss&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\([y-f(x)] = [1-yf(x)]^2\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(2Pr(Y=+1|x)-1\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;easy cross validation of regularization parameters/slower convergence rates&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Hinge loss&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\([1-yf]_+\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(sign[Pr(Y=+1|x)-\frac{1}{2}]\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;support points/not differentiable at&lt;span class=&#34;math inline&#34;&gt;\(yf(x)=1\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Exponential loss&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac {1}{2} log(1+e^{-Yf(x)})\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{2}log\frac{Pr(Y =+1|x)}{Pr(Y =-1|x)}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;grows exponentially for negative values,more sensitive to outliers&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Logistic loss&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(log(1+e^{-Yf(x)})\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(log\frac{Pr(Y =+1|x)}{Pr(Y =-1|x)}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;grows linearly for negative values,less sensitive to outliers&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Similarities between loss functions: &lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Hinge loss, Exponential loss, Logistic loss have very similar tails, giving zero penalty to points well inside their margin and linear or exponential penalty to points on the wrong side adn far away. &lt;span style=&#34;color:FireBrick&#34;&gt;Squared error loss&lt;/span&gt; gives a quadratic penalty and points inside their own margin have a strong influence on othe model as well.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Exponential loss and Logistic loss have the same asymptotes as the SVM hinge loss but are rounded in the interrior.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br /&gt;
The above popular loss functions are also used in deep learning, for example, Learing To Rank (&lt;a href=&#34;https://en.wikipedia.org/wiki/Learning_to_rank&#34;&gt;LTR&lt;/a&gt;) for a Recommender System (&lt;a href=&#34;https://en.wikipedia.org/wiki/Recommender_system&#34;&gt;RS&lt;/a&gt;). Differently from traditional machine learning problem that’s to predict the target either classification or regression, LTR optimizes the ranking accruacy instead of the prediction probability accuracy.&lt;/p&gt;
&lt;p&gt;Ranking is useful in our daily life for Recommendation system like Netflix, Amazon; Information Retrieval like goole; Drug discovery; Bioinformatics. Generally speaking, there are three types of rankings: &lt;em&gt;bipartie ranking, k-partite ranking, real-valued labels based ranking&lt;/em&gt;. &lt;code&gt;RankSVM, RankBoost, RankNet&lt;/code&gt; with corresponding loss functions are used for the ranking problems. A seperate post will be written to further demonstrate LTR framework and how the loss functions are used.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;
&lt;em&gt;Reference:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Hastie, T., Tibshirani, R., &amp;amp; Friedman, J. H. (2009). The elements of statistical learning: data mining, inference, and prediction. 2nd ed. New York: Springer.&lt;/em&gt;
&lt;em&gt;Computer Science &amp;amp; Artificial Intelligence Laboratory, MIT&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Game Theory and Decision Theory</title>
      <link>https://yuan-du.com/post/2020-09-23-decision-theory/decision-theory/</link>
      <pubDate>Wed, 23 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://yuan-du.com/post/2020-09-23-decision-theory/decision-theory/</guid>
      <description>


&lt;p&gt;Statistics starts with probability theory, particularly in the analysis of games of chance. To be refferred to as a game, it involves three elements mathmatically:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Parameter space &lt;span class=&#34;math inline&#34;&gt;\(\Theta\)&lt;/span&gt;, a population characteristics, a physical quantity for example, mean.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Actions/Decisions space &lt;span class=&#34;math inline&#34;&gt;\(\mathscr{D}\)&lt;/span&gt; available to statistician.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A loss function, &lt;span class=&#34;math inline&#34;&gt;\(L(\theta,d)\)&lt;/span&gt;, a real-valued function defined on &lt;span class=&#34;math inline&#34;&gt;\(\Theta \times \mathscr{D}\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Thus, any such triplet &lt;span class=&#34;math inline&#34;&gt;\((\Theta, \mathscr{D}, L )\)&lt;/span&gt; defines a game. For example, Black jack, poker, chess, tic-tac-toe and so on are games that are played by strategy. &lt;a href=&#34;https://en.wikipedia.org/wiki/Game_theory&#34;&gt;“Game Theory”&lt;/a&gt; was proposed by two economists: John Von Neuman and John Nash in 1950s. Two or more players competing against one another. Neither player generally knows the others’ strategy. The goal of the game is to pick a strategy that guarentees he/she can’t be “too bad”.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Real life examples: &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Product pricing decisions: &lt;/strong&gt; Seasonal promotions allow retailers to sell more stock of products and consumers to get best deals. The focus of retailers is on using the best pricing strategy while the preference of consumers is to choose the best deal in terms of discount and variety.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Investment decisions: &lt;/strong&gt; The different distributions of the investment on bond, stocks, short-term reserves will result in different returns. A historical risk/return (1926-2018) can be found at &lt;a href=&#34;https://personal.vanguard.com/us/insights/saving-investing/model-portfolio-allocations&#34;&gt;Vanguard portfolio allocation models&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Prisoners’ dilemma: &lt;/strong&gt;The moral of the story in terms of decisions in a legal setting: &lt;code&gt;You have the right to stay silent and please shut the f* up and let your attoney to do the talk&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yuan-du.com/img/Game-Theory.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://studiousguy.com/game-theory-examples/#4_Product_Pricing_Decisions&#34;&gt;More examples&lt;/a&gt; can be found in this post.&lt;/p&gt;
&lt;p&gt;Decision theory is similar to the game theory. The main differences are :&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;In the statistical context, the players are the statistician and “Nature”, who knows the true value of the parameter. In two-player game, both are trying simultaneously to maximize their winnings, whereas in decision theory nature chooses a state without this view in mind.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;All statistical games allow statistician to gain information by sampling. However, it is the exploitation of the structure which such gathering of information gives to a game that distinguishes decision theory from game theory proper.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;A real life example:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Medical diagnosis:&lt;/code&gt; Sometimes you never know until you open up the patient to see if the cancer is absent because of the limitations on imaging diagnosis. A surgeon needs to decide if a surgery (an action/ a decision) is necessary based on if the patient has cancer or not. There are 4 combinations between the 2 decisions and 2 conditions, thus 4 outcomes scored by %.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:green&#34;&gt;Combination 1&lt;/span&gt;: The presence of cancer is confirmed and the surgeon decides to perform a surgery. The score is 100% because that’s the best decision.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:red&#34;&gt;Combination 2&lt;/span&gt;: There is presence of cancer and the surgoen decides not to perform a surgery. The score is 0% because that’s the worst consequence.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:brown&#34;&gt;Combination 3&lt;/span&gt;: Cancer is absent and the surgeon decides to perform a surgery. The score is 40% because it doesn’t results in serious consequence.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:blue&#34;&gt;Combination 4&lt;/span&gt;: Cancer is absent and surgoen decides not to perform a surgery. The score is 85% because it’s a good decision and no consequence as well.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yuan-du.com/img/Decision-Theory.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;Models, Decision rules and Risk&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Statistical model (class or family of distributions):&lt;/strong&gt; The parameter &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; and data &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; are related through a model in which the distribution of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is determined by &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; The distribution when the parameter &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is denoted &lt;span class=&#34;math inline&#34;&gt;\(P_\theta\)&lt;/span&gt; and we write &lt;span class=&#34;math inline&#34;&gt;\(X \sim P_\theta\)&lt;/span&gt;. Formally, a model is written as the set of distributions for &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\mathscr{P} = {P_\theta: \theta \in \Theta}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Decision rules:&lt;/strong&gt; a non-randomized decision rule is a function &lt;span class=&#34;math inline&#34;&gt;\(\delta : \mathscr{X} -&amp;gt; \mathscr{D}\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The set of decision rules: &lt;span class=&#34;math inline&#34;&gt;\(\mathscr{D} =&amp;gt; d \in \mathscr{D}, \theta \in \mathscr{D}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\delta(x) \in \mathscr{D}\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(d=\delta(x)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt; prescribes a course of action for every observable dataset &lt;span class=&#34;math inline&#34;&gt;\(x\in \mathscr{X}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Risk:&lt;/strong&gt; to evaluate a decision rule, we use &lt;strong&gt;risk &lt;span class=&#34;math inline&#34;&gt;\(R(\theta, \delta)\)&lt;/span&gt;&lt;/strong&gt;. It is the average &lt;strong&gt;loss &lt;span class=&#34;math inline&#34;&gt;\(L(\theta,d)\)&lt;/span&gt;&lt;/strong&gt; between the estimand &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; and the estimator &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; –&amp;gt; an expected loss &lt;span class=&#34;math inline&#34;&gt;\(E_\theta\{ L[\theta,\delta(X)] \}\)&lt;/span&gt;. One wants the estmator &lt;span class=&#34;math inline&#34;&gt;\(\delta(x)\)&lt;/span&gt; to be accurate, but just what measure of accuracy should be used is fairly arbitrary. Mean squared error (MSE) is the most famous measure. In 1820s, Gauss proposed the square of the error as a measure of loss. He defends his choice by an appeal to mathmematical simplicity and convenience.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;
&lt;em&gt;Reference:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;1. Mathematical Statistics: A Decision Theoretic Approach by Thomas S. Ferguson, Academic Press; 1st edition&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;2. Theoretical Statistics: Topics for a Core Course by Robert W. Keener, Springer; 2010 edition&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;3. Theory of Point Estimation by Erich L. Lehmann, George Casella, Springer; 2nd edition&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Research Experience - Comparing Statistics vs Machine Learning</title>
      <link>https://yuan-du.com/post/2020-07-30-statvsml/statisticsvsml/</link>
      <pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://yuan-du.com/post/2020-07-30-statvsml/statisticsvsml/</guid>
      <description>
&lt;link href=&#34;https://yuan-du.com/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://yuan-du.com/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;There are so many terms regarding the field of Statistics and Data Science. We often heard Statistics, Data Mining, Machine Learning, Big Data, etc. It especially confuses people that’s in a different field. I remember that over five years ago, a radiologist asked me if I can mine data from the radiology system because she saw that I have Data Mining skills. I was blown away by the understanding of Data Mining to a doctor. Data Mining and data extraction is totally different. After data extraction and data preparation, data mining is used to identify patterns and relationships based on the research/business questions.&lt;/p&gt;
&lt;p&gt;Generally speaking, due to the storage and advancement of computers, our data analysis power which builds on Statistical knowledge expanded by using more complicated statistical theory and algorithms that are applied to multidisciplinary science such as Biostatistics, Medicine, Public Health, Computer Science, Engineering, Physicis, etc.
&lt;img src=&#34;https://yuan-du.com/img/History-DM.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Nature has a paper &lt;a href=&#34;https://www.nature.com/articles/nmeth.4642#:~:text=Statistics%20draws%20population%20inferences%20from,learning%20finds%20generalizable%20predictive%20patterns.&amp;amp;text=Two%20major%20goals%20in%20the,systems%20are%20inference%20and%20prediction.&#34;&gt;“Statistics versus machine learning”&lt;/a&gt; that explains the relationships.&lt;a href=&#34;https://www.aaai.org/ojs/index.php/aimagazine/article/view/1230/1131&#34;&gt;From Data Mining to Knowledge Discovery in Databases&lt;/a&gt; discussed and summrized the history of Knowledge discovery of database (KDD).&lt;/p&gt;
&lt;p&gt;In the realm of healthcare research studies, I would like to share my own experience of what types of statistical learning were used. Based on the objectives of a study, we generally have two types of goals:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Inference&lt;/strong&gt;: Identify risk factors that associate with response outcome(s). It normally has smaller sample size. This is the most common goal in medical research. It requires clinical knowledge to start with research questions that involve hypothesis. Univariate analysis (Hypothesis testing) and Multivariable analysis are used. Both types of analysis need assumptions on the data distribution, variance and linear/nonlinear relationship with response variable(s) to perform correct statistical tests. For univariate analysis, please check out my slides for the most commonly used &lt;a href=&#34;https://yuan-du.com/slides/2019-09-advancedstat&#34;&gt;hypothesis testings&lt;/a&gt;. The most common problem is &lt;code&gt;significance (p-value) fishing&lt;/code&gt;. There are difference p-value adjustment methods to consider when there are multiple testings. Physicians/researchers often want to publish significant testing result only which is not healthy for medical research. Non significant factors are important to the literature. It’s useful for meta analysis. For multivariable analysis, here are some examples that difference statistical models were used:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Clinical Outcome Study&lt;/strong&gt;: &lt;a href=&#34;https://yuan-du.com/publication/2017-1-copd/&#34;&gt;Comparison of hospital outcomes and resource utilization in acute COPD exacerbation patients managed by teaching versus non-teaching services in a community hospital&lt;/a&gt;: The data was from both national database &lt;a href=&#34;https://www.premierinc.com/about&#34;&gt;Premier&lt;/a&gt; and hospital EHR(Electric Health Record). &lt;code&gt;Multiple logistic regression&lt;/code&gt; was used for the multivariable analysis to identify the factors that contribute to resource utilization in the acute COPD patients.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Commercial Device Study&lt;/strong&gt;: &lt;a href=&#34;https://yuan-du.com/publication/2020-4-pharm/&#34;&gt;Characterisation of ICU sleep by a commercially available activity tracker and its agreement with patient-perceived sleep quality&lt;/a&gt;: This data was collected from ICU patients that used fitbits as alternative sleep tracking devices. Since each patient was measured several times, a &lt;code&gt;mixed model repeated measure&lt;/code&gt; was used to detect the correlation/agreement between each sleep quality measure and the gold standard Richard-Campbell Sleep Questionnaire (RCSQ). Instead of a single pearson correlation coefficient, the &lt;code&gt;bootstrap method&lt;/code&gt; with 1000 times was implemented to generate Confidence Interval for statistical inference.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Population Health Study&lt;/strong&gt;: &lt;a href=&#34;https://yuan-du.com/publication/2015-9-rhc/&#34;&gt;Contextual, Organizational and Ecological Effects on the Variations in Hospital Readmissions of Rural Medicare Beneficiaries in Eight Southeastern States&lt;/a&gt;: This a longitudinal study funded by NIH. Data was from mainly from CMS data warehouse on beneficiaries and providers. First, &lt;code&gt;risk-adjusted readmission&lt;/code&gt; was calculated by Logistic regression model on patient level. Then Generalized Estimating Equation (&lt;code&gt;GEE&lt;/code&gt;) method was performed on the rurual clinic level for 6 years of data. This is a type of &lt;code&gt;hierarchical regression&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If the number of variables is very large compared to observations (p&amp;gt;n), for example genomics, a person has hundreds of genes. Or when the ratio of p/n is larger than normal and the linear/nonlinear relationships and assumptions are vague, noval machine learning methods are preferred.&lt;/p&gt;
&lt;p&gt;One example is the &lt;a href=&#34;https://yuan-du.com/talk/2019-11-25-breast-cancer-by-svm/&#34;&gt;breast cancer tumor classification&lt;/a&gt;. Another example is a Leukemia project that i’m currently working on to identify unknown gene mutation effects to the mortality of the patients. There are only 125 patients, and each patient has over 38 gene mutations. The gene mutations are sparse. Methods with penalty and constraints will be suitable for this type of data. I’ll discuss more about this project seperately later.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Prediction&lt;/strong&gt;: Predict outcomes. It preferrs big sample size for better prediction accuracy.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Covid 19 Study&lt;/strong&gt; &lt;a href=&#34;https://pubmed.ncbi.nlm.nih.gov/27544541/&#34;&gt;This paper&lt;/a&gt; was reference for prediciton: Due to the extensive research studies on Covid 19. Our hospital identified various data and interesting risk factors to predict Covid 19 positive cases. On one hand, the study aims to identify additional risk factors. and on the other hand, with over 10K patients’ data, the study aims to predict Covid 19 cases based on the massive data. &lt;code&gt;Multiple logistic regression&lt;/code&gt;, &lt;code&gt;Random Forest&lt;/code&gt;, and &lt;code&gt;XGboost&lt;/code&gt; were used to predict the outcome. Since the risk factors and response variable have more linear relationship, and with a better interpretability, &lt;code&gt;Multiple logistic regression&lt;/code&gt; with training and validation test was picked and each patient has a risk score for decision makers to utilize the hospital resources.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Closing Note&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In healthcare research, asking the right questions and have clinical knowledge is very essential to determine the patient population and appropriate methods. Understanding the problems and using the efficient methods provides a strong solution. Statistical inference is essential in traditional Health care research. Maching learning method is more flexible and is generally better for prediction, big data or unknown assumptions.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
