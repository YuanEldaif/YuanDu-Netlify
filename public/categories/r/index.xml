<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R | Yuan Du</title>
    <link>https://yuan-du.com/categories/r/</link>
      <atom:link href="https://yuan-du.com/categories/r/index.xml" rel="self" type="application/rss+xml" />
    <description>R</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© Yuan Du, 2021 </copyright><lastBuildDate>Sat, 18 Apr 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://yuan-du.com/img/icon-192.png</url>
      <title>R</title>
      <link>https://yuan-du.com/categories/r/</link>
    </image>
    
    <item>
      <title>Bayesian Analysis</title>
      <link>https://yuan-du.com/post/2020-04-18-bayesian-analysis/bayesian-analysis/</link>
      <pubDate>Sat, 18 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://yuan-du.com/post/2020-04-18-bayesian-analysis/bayesian-analysis/</guid>
      <description>


&lt;p&gt;Bayesian approach becomes more and more popular because of the improvement of the mordern computing ability for machine learning and big data. Bayesian analysis is a completely different approach compared to frequentist approach. Yet, it’s more challenging. To be able to understand and learn the Bayesian approach, &lt;code&gt;first&lt;/code&gt;, we will need to have the knowledge of conditional probability. &lt;code&gt;Second&lt;/code&gt;, to have the knowledge of different distributions such as normal, bernoulli, binomial, gamma, beta, cauchy, possion, etc. &lt;code&gt;Third&lt;/code&gt;, to be familiar with calculus. We need to calculate derivatives and integration of different distributions. &lt;code&gt;Last but not at least&lt;/code&gt;, to be familiar with simulation sampling techniques such as Grid sampling, Variational Bayes and Monte Carlo Markov Chian (MCMC) including popular Gibbs sampling, Metropolis Hastings Sampling, Hamiltonian Monte Carlo (HMC),etc. Luckily, there are a few software tools &lt;a href=&#34;https://web.sgh.waw.pl/~atoroj/ekonometria_bayesowska/jags_user_manual.pdf&#34;&gt;Jags&lt;/a&gt;, &lt;a href=&#34;https://mc-stan.org/users/documentation/&#34;&gt;Stan&lt;/a&gt; .etc can be used for Gibbs sampling and HMC.
&lt;img src=&#34;https://yuan-du.com/img/Human.jpg&#34; alt=&#34;‘Credit: https://www2.isye.gatech.edu/~brani/isyebayes/jokes.html’&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The concept is simple. According to Bayes Theorem &lt;span class=&#34;math inline&#34;&gt;\(p(\theta|y) = \frac{p(y|\theta)p(\theta)}{p(y)}\)&lt;/span&gt; or without normalization &lt;span class=&#34;math inline&#34;&gt;\(p(\theta|y) \sim p(y|\theta)p(\theta)\)&lt;/span&gt;, we want to gain the posterior distribution by using prior information and likelihood of the data information. Most of the time, we use log likelihood instead for easier calculation since &lt;span class=&#34;math inline&#34;&gt;\(log( a*b )= loga +logb\)&lt;/span&gt;. Bayesian approach involves much more math than frequentist approach and more complicated. Frequentist focus on one point estimate (hypothesis and Confidence Interval), and bayesian focuses on a looking forward range (Credible Interval).
&lt;img src=&#34;https://yuan-du.com/img/FreBay.png&#34; alt=&#34;Credit: https://365datascience.com/bayesian-vs-frequentist-approach/&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If friquentist and bayesian approach are just two different ways to look into things, Why and when is recommended to use Bayesian approach instead of frequentist approach?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Here are a few examples to use Bayesian approach over frequentist approach:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Clear prior information: the example in the book &lt;a href=&#34;http://www.stat.columbia.edu/~gelman/book/BDA3.pdf&#34;&gt;BD3&lt;/a&gt; of Andrew Gelman’s in Chapter 1, problem 6. The prior information is that approximately 1/125 of all births are fraternal twins and 1/300 of births are identical twins.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Seperation problems. For example logistic regression couldn’t converge due to high dimension and small sample size.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Estimate multiple outcomes with credible interval. For example, family doctors try to diagnosis diseases (such as cold, flu) based on multiple symptoms (such as headache, sore throat, high temprature) and the probabilities of all symptoms sum up to limited possible diseases.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Small sample size with multiple experiments and limited budget.It’s a preferred meta analysis than tranditional meta analysis that has high heterogeneity with different recources since it provides a credible interval instead of confident interval.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;Reference:&lt;/code&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://www.stat.columbia.edu/~gelman/stuff_for_blog/ohagan.pdf&#34;&gt;Dicing with the unknown&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.fda.gov/regulatory-information/search-fda-guidance-documents/guidance-use-bayesian-statistics-medical-device-clinical-trials#3&#34;&gt;FDA Guidance for the Use of Bayesian Statistics in Medical Device Clinical Trials&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Awesome data science presentations and tools</title>
      <link>https://yuan-du.com/post/tools-reference/</link>
      <pubDate>Fri, 16 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://yuan-du.com/post/tools-reference/</guid>
      <description>&lt;p&gt;Recently, I discovered a few pretty cool tools, R-based and easy to follow.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;R Presentation Themes&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Why uses R presentation?&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Take advantage of &lt;a href=&#34;https://yuandueldaif.netlify.com/post/rmarkdown_intro/&#34; target=&#34;_blank&#34;&gt;R Markdown&lt;/a&gt; .&lt;/strong&gt; You can write all your slides in Markdown text&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Power of interchange.&lt;/strong&gt; You can include chunks of R code and rendered output like plots, results, tables, etc. in your slides&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Version control and sharing.&lt;/strong&gt; You can use git for version control and share your GitHub repository&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;My current favorate &lt;a href=&#34;https://github.com/yihui/xaringan/wiki/Themes&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Xaringan&lt;/strong&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/emitanaka/ninja-theme&#34; target=&#34;_blank&#34;&gt;nanja-theme&lt;/a&gt; was developed by Emi Tanaka.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Data Cleaning/Data Wrangling tool kit - Tidyverse&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Book&lt;/strong&gt; &lt;a href=&#34;https://r4ds.had.co.nz/&#34; target=&#34;_blank&#34;&gt;R for Data Science&lt;/a&gt; by Garrett Grolemund &amp;amp; Hadley Wickham.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Visual&lt;/strong&gt; Data cleaning post- &lt;a href=&#34;https://jules32.github.io/2016-07-12-Oxford/dplyr_tidyr/#3_tidyr_overview&#34; target=&#34;_blank&#34;&gt;Tidyverse overview&lt;/a&gt; by Julia Stewart Lowndes.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;I&amp;rsquo;m finishing up on an advanced statistical class &lt;a href=&#34;https://yuandueldaif.netlify.com/talk/2019-09-advancedstat/&#34; target=&#34;_blank&#34;&gt;&amp;ldquo;Significance Test&amp;rdquo;&lt;/a&gt; by using R presentation and will share the code on Github.
&lt;br/&gt;so stay tuned&amp;hellip;&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tidy Text Mining</title>
      <link>https://yuan-du.com/post/tidy-text-mining/</link>
      <pubDate>Fri, 19 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://yuan-du.com/post/tidy-text-mining/</guid>
      <description>


&lt;p&gt;Thanks to the short course at SDSS 2019, I learned how to do tf-idf to topic modeling and sentiment analysis by using tidytext taught by Julia Silge, author of &lt;a href=&#34;https://www.tidytextmining.com/&#34;&gt;Text Mining with R&lt;/a&gt; and Mara Averick. They did a great job on teaching the four hour class. I didn’t expect to have so much covered in the short course.&lt;/p&gt;
&lt;p&gt;Here is an example that I used the method to analyze &lt;strong&gt;A Tale of Two Cities&lt;/strong&gt; and &lt;strong&gt;Great Expectations&lt;/strong&gt; by Charles Dickens by using sentiment analysis.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Install the &lt;code&gt;tidytext&lt;/code&gt; package for text mining.&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;install.packages(&#34;tidytext&#34;)&lt;/code&gt;&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Read the book from gutenbergr package, after install gutenbergr package&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We can Downloading books by ID (98 and 1400) from &lt;a href=&#34;http://www.gutenberg.org/&#34;&gt;Project Gutenbergr&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(gutenbergr)
library(dplyr)

book &amp;lt;-  gutenberg_download(c(98, 1400), meta_fields = &amp;quot;title&amp;quot;) %&amp;gt;%
  group_by(title) %&amp;gt;%
  mutate(line = row_number()) %&amp;gt;%
  ungroup()

book&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 35,889 x 4
##    gutenberg_id text                             title                 line
##           &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;                            &amp;lt;chr&amp;gt;                &amp;lt;int&amp;gt;
##  1           98 A TALE OF TWO CITIES             A Tale of Two Cities     1
##  2           98 &amp;quot;&amp;quot;                               A Tale of Two Cities     2
##  3           98 A STORY OF THE FRENCH REVOLUTION A Tale of Two Cities     3
##  4           98 &amp;quot;&amp;quot;                               A Tale of Two Cities     4
##  5           98 By Charles Dickens               A Tale of Two Cities     5
##  6           98 &amp;quot;&amp;quot;                               A Tale of Two Cities     6
##  7           98 &amp;quot;&amp;quot;                               A Tale of Two Cities     7
##  8           98 CONTENTS                         A Tale of Two Cities     8
##  9           98 &amp;quot;&amp;quot;                               A Tale of Two Cities     9
## 10           98 &amp;quot;&amp;quot;                               A Tale of Two Cities    10
## # ... with 35,879 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Process books into chapters and words in tidy data form&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;we need to restructure it as one-token-per-row format. As pre-processing, we divide these into chapters, use tidytext’s &lt;code&gt;unnest_tokens&lt;/code&gt; to separate them into words, then remove &lt;code&gt;stop_word&lt;/code&gt;s. We’re treating every chapter as a separate “document”, each with a name like &lt;em&gt;A Tale of Two cities&lt;/em&gt; or &lt;em&gt;Great Expectations&lt;/em&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidytext)
tidy_book &amp;lt;- book %&amp;gt;%
  unnest_tokens(word, text)

tidy_book&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 323,972 x 4
##    gutenberg_id title                 line word  
##           &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;                &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt; 
##  1           98 A Tale of Two Cities     1 a     
##  2           98 A Tale of Two Cities     1 tale  
##  3           98 A Tale of Two Cities     1 of    
##  4           98 A Tale of Two Cities     1 two   
##  5           98 A Tale of Two Cities     1 cities
##  6           98 A Tale of Two Cities     3 a     
##  7           98 A Tale of Two Cities     3 story 
##  8           98 A Tale of Two Cities     3 of    
##  9           98 A Tale of Two Cities     3 the   
## 10           98 A Tale of Two Cities     3 french
## # ... with 323,962 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidy_book &amp;lt;- tidy_book %&amp;gt;%
  anti_join(get_stopwords())

#We can also use count to find the most common words in all the book as a whole
tidy_book %&amp;gt;%
  count(word, sort = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 14,594 x 2
##    word       n
##    &amp;lt;chr&amp;gt;  &amp;lt;int&amp;gt;
##  1 said    2010
##  2 mr      1333
##  3 one      940
##  4 now      715
##  5 joe      698
##  6 upon     655
##  7 time     640
##  8 little   638
##  9 miss     616
## 10 know     613
## # ... with 14,584 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Sentiment analysis&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Sentiment analysis can be done as an inner join. Three sentiment lexicons are available via the &lt;code&gt;get_sentiments()&lt;/code&gt; function. Let’s examine how sentiment changes during each novel. Let’s find a sentiment score for each word using the Bing lexicon, then count the number of positive and negative words in defined sections of each novel.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyr)
get_sentiments(&amp;quot;bing&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6,786 x 2
##    word        sentiment
##    &amp;lt;chr&amp;gt;       &amp;lt;chr&amp;gt;    
##  1 2-faces     negative 
##  2 abnormal    negative 
##  3 abolish     negative 
##  4 abominable  negative 
##  5 abominably  negative 
##  6 abominate   negative 
##  7 abomination negative 
##  8 abort       negative 
##  9 aborted     negative 
## 10 aborts      negative 
## # ... with 6,776 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sentiment &amp;lt;- tidy_book %&amp;gt;%
  inner_join(get_sentiments(&amp;quot;bing&amp;quot;), by = &amp;quot;word&amp;quot;) %&amp;gt;% 
  count(title, index = line %/% 80, sentiment) %&amp;gt;% 
  spread(sentiment, n, fill = 0) %&amp;gt;% 
  mutate(sentiment = positive - negative)

sentiment&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 450 x 5
##    title                index negative positive sentiment
##    &amp;lt;chr&amp;gt;                &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
##  1 A Tale of Two Cities     0        5        9         4
##  2 A Tale of Two Cities     1       28       28         0
##  3 A Tale of Two Cities     2       27       15       -12
##  4 A Tale of Two Cities     3       17       10        -7
##  5 A Tale of Two Cities     4       13       12        -1
##  6 A Tale of Two Cities     5       19       18        -1
##  7 A Tale of Two Cities     6       24       16        -8
##  8 A Tale of Two Cities     7       24       19        -5
##  9 A Tale of Two Cities     8        9       27        18
## 10 A Tale of Two Cities     9       29       23        -6
## # ... with 440 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Now we can plot these sentiment scores across the plot trajectory of each novel.&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)

ggplot(sentiment, aes(index, sentiment, fill = title)) +
  geom_bar(stat = &amp;quot;identity&amp;quot;, show.legend = FALSE) +
  facet_wrap(~title, ncol = 2, scales = &amp;quot;free_x&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yuan-du.com/post/2019-07-18-tidy-text-mining/index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It looks like that &lt;strong&gt;A Table of Two Cities&lt;/strong&gt; has more negative emotions and &lt;strong&gt;Great Expectations&lt;/strong&gt; is more balanced on emotions.&lt;/p&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Most common positive and negative words&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;One advantage of having the data frame with both sentiment and word is that we can analyze word counts that contribute to each sentiment.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bing_word_counts &amp;lt;- tidy_book %&amp;gt;%
  inner_join(get_sentiments(&amp;quot;bing&amp;quot;)) %&amp;gt;%
  count(word, sentiment, sort = TRUE)

bing_word_counts&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2,575 x 3
##    word   sentiment     n
##    &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;     &amp;lt;int&amp;gt;
##  1 miss   negative    616
##  2 like   positive    541
##  3 well   positive    483
##  4 good   positive    473
##  5 great  positive    360
##  6 better positive    221
##  7 right  positive    172
##  8 poor   negative    164
##  9 dark   negative    160
## 10 work   positive    152
## # ... with 2,565 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This can be shown visually, and we can pipe straight into ggplot2 because of the way we are consistently using tools built for handling tidy data frames.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bing_word_counts %&amp;gt;%
  filter(n &amp;gt; 100) %&amp;gt;%
  mutate(n = ifelse(sentiment == &amp;quot;negative&amp;quot;, -n, n)) %&amp;gt;%
  mutate(word = reorder(word, n)) %&amp;gt;%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col() +
  coord_flip() +
  labs(y = &amp;quot;Contribution to sentiment&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yuan-du.com/post/2019-07-18-tidy-text-mining/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;
This lets us spot an anomaly in the sentiment analysis; the word “miss” is coded as negative but it is used as a title for young, unmarried women in Jane Austen’s works. If it were appropriate for our purposes, we could easily add “miss” to a custom &lt;code&gt;stop-words&lt;/code&gt; list using &lt;code&gt;bind_rows&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;custom_stop_words &amp;lt;- bind_rows(get_stopwords(),
                               tibble(word = &amp;quot;miss&amp;quot;,
                                          lexicon = &amp;quot;custom&amp;quot;))

tidy_book2 &amp;lt;- tidy_book %&amp;gt;%
  anti_join(custom_stop_words) %&amp;gt;%
  count(word, sort = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;6&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Wordclouds&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We’ve seen that this tidy text mining approach works well with ggplot2, but having our data in a tidy format is useful for other plots as well.&lt;/p&gt;
&lt;p&gt;For example, consider the wordcloud package. Let’s look at the most common words in Charles Dickens’ two books as a whole again.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(wordcloud)

tidy_book %&amp;gt;%
  count(word) %&amp;gt;%
  with(wordcloud(word, n, max.words = 100))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yuan-du.com/post/2019-07-18-tidy-text-mining/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In other functions, such as comparison.cloud, you may need to turn it into a matrix with &lt;code&gt;reshape2&lt;/code&gt;’s acast. Let’s do the sentiment analysis to tag positive and negative words using an inner join, then find the most common positive and negative words. Until the step where we need to send the data to comparison.cloud, this can all be done with joins, piping, and dplyr because our data is in tidy format.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(reshape2)

tidy_book %&amp;gt;%
  inner_join(get_sentiments(&amp;quot;bing&amp;quot;)) %&amp;gt;%
  count(word, sentiment, sort = TRUE) %&amp;gt;%
  acast(word ~ sentiment, value.var = &amp;quot;n&amp;quot;, fill = 0) %&amp;gt;%
  comparison.cloud(colors = c(&amp;quot;#F8766D&amp;quot;, &amp;quot;#00BFC4&amp;quot;),
                   max.words = 100)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yuan-du.com/post/2019-07-18-tidy-text-mining/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;For “Converting to and from Document-Term Matrix and Corpus objects”, You can visit &lt;a href=&#34;https://cran.r-project.org/web/packages/tidytext/vignettes/tidying_casting.html&#34;&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
