---
title: "Breast Cancer Image Diagnosis Classification by SVM"
author: "Yuan Du"
date: "12/5/2019"
output:
    html_document: 
    df_print: paged
    fig_caption: true
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)
library(ggplot2)
library(psych)
library(corrplot)
library(e1071)
library(caret)
library(plotly)
library(plyr)

```
# Introduction
‚ö†Ô∏èBreast cancer is the second leading cause of cancer death in women after lung cancer. 

‚ö†Ô∏èThere is a 1 in 8 chance she will develop breast cancer in the United States.

‚ö†Ô∏èThe average 5-year survival rate for women with invasive breast cancer is 90%.

üî¥If the cancer is located only in the breast, the 5-year survival rate of women with breast cancer is 99%. 62% of people with breast cancer are diagnosed with this stage.

üî¥If the cancer has spread to the regional lymph nodes, the 5-year survival rate is 85%.

üî¥If the cancer has spread to a distant part of the body, the 5-year survival rate is 27%.

## Dataset description:
Breast Cancer Wisconsin (Diagnostic) Data Set by University of Wisconsin, Clinical Sciences Center was used. There are 569 number of instances, and 32 attributes.First two columns are ID number and Diagnosis (M = malignant, B = benign). Ten real-valued features are computed for each cell nucleus:
a) radius (mean of distances from center to points on the perimeter)\ 
b) texture (standard deviation of gray-scale values)\ 
c) perimeter\ 
d) area\ 
e) smoothness (local variation in radius lengths)\ 
f) compactness (perimeter^2 / area - 1.0)\ 
g) concavity (severity of concave portions of the contour)\ 
h) concave points (number of concave portions of the contour)\ 
i) symmetry\ 
j) fractal dimension ("coastline approximation" - 1)\

The mean, standard error and "worst" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.
```{r, fig.cap='Figure 1: Breast cancer cell'}
knitr::include_graphics('Cell.jpg')
```


## Data Problems:
High dimensions: There are there are 32 attributes for 569 number of instances, which is considered high dimensions for these instances.Another major problem is multicolinearity. The same feature for each image was computed in three ways - mean, standard error and "worst" or largest (mean of the three largest values), results in 30 variables.\

# Methodology
SVM with radial basis function (RBF) kernel was used to analyze the data, since it's a classification problem and SVM is a good method for data with high dimensions. SVM Estimator $$S_n: (\mathcal{X} \, X \, \mathcal{Y})^n \to H, \quad D_n \mapsto f_{L,D_n,\lambda_{D_n}}$$

, where $f_{L,D_n,\lambda_{D_n}}$ is that function $f \in H$ which minimizes $$\frac{1}{n} \sum L(x_i,y_i,f(x_i)) + \lambda_{D_n}||f||_H^2$$.

## Investiagte the Asymptotic properties on estimators:

Assumptions to prove that Empirical SVM $f_{L,D_n,\lambda_{D_n}}$ is converged to theoretical SVM $f_{L,P,\lambda_0}$. This is $$\sqrt{n}(f_{L,D_n,\lambda_{D_n}} - f_{L,P,\lambda_0})$$ converges weakly to a (zero-mean) Gaussian process in the function space H.:

1) $\mathcal{X}$ bounded and closed, $\mathcal{Y} \in \{-1,1\}$ in Function space H, $f:\mathcal{X} \to \mathbb{R}$, $(X_1,Y_1),\dots,(X_n,Y_n)$ are independent and identically distributed according to some unknown probability measure P on $(\mathcal{X} X \mathcal{Y},\mathcal{B}(\mathcal{X} X \mathcal{Y}))$.\


2) Assume Loss function is two times continuously differentiable in the third argument. This assumption is not based on any unknown entity as such model distribution P.\

3) Assume L is a P-integrable Nemitski loss function.\


4) $\lambda$ is random sequence, so it's possible to use any data-driven method for choosing the regulization parameter, such as cross-validation. It's desirable to have a bound $$0 \leq \mathcal{R}_{L,P}(f_{L,P,\lambda_0}) - \mathcal{R}_{L,P}^* \leq C(\lambda_0) \to 0 \text{for} \lambda \to 0$$ to find $\lambda$.\

Asymptotic properties of support vector machines are investigated. 
It is shown that the difference between the empirical and the theoretical SVM is asymptotically normal with rate $\sqrt{n}$; that is, $\sqrt{n}(f_{L,D_n,\lambda_{D_n}} - f_{L,P,\lambda_0})$ converges to a Gaussian process in the function space H.\

These results are not only of theoretical interest but also be a starting point for statistical inferences such as confidence intervals and hypothesis testing.\

Without any of such assumptions,similar results are not possible as follows from the no-free-lunch theorem.

# Results
```{r read, message=FALSE, warning=FALSE, include=FALSE}
Data <- read_csv("C:/Work/Project/STA7734/SVM-Asymptotic-Normality/Data/data.csv",
                 col_names = T,
                 col_types=cols(ID =col_character()Ôºå
                                Diagnosis=col_character(),
                                "3" = col_double(),
                                "4" = col_double(),
                                "5" = col_double(),
                                "6" = col_double(),
                                "7" = col_double(),
                                "8" = col_double(),
                                "9" = col_double(),
                                "10" = col_double(),
                                "11" = col_double(),
                                "12" = col_double(),
                                "13" = col_double(),
                                "14" = col_double(),
                                "15" = col_double(),
                                "16" = col_double(),
                                "17" = col_double(),
                                "18" = col_double(),
                                "19" = col_double(),
                                "20" = col_double(),
                                "21" = col_double(),
                                "22" = col_double(),
                                "23" = col_double(),
                                "24" = col_double(),
                                "25" = col_double(),
                                "26" = col_double(),
                                "27" = col_double(),
                                "28" = col_double(),
                                "29" = col_double(),
                                "30" = col_double(),
                                "31" = col_double(),
                                "32" = col_double()
                                )) %>%
                        dplyr::rename( "radius" = "3",
                                "texture" = "4",
                                "perimeter" = "5",
                                "area" = "6",
                                "smoothness" = "7",
                                "compactness" = "8",
                                "concavity" = "9",
                                "concave_points" = "10",
                                "symmetry" = "11",
                                "fractal_dimension" = "12",
                                "radiusSE" = "13",
                                "textureSE" = "14",
                                "perimeterSE" = "15",
                                "areaSE" = "16",
                                "smoothnessSE" = "17",
                                "compactnessSE" = "18",
                                "concavitySE" = "19",
                                "concave_pointsSE" = "20",
                                "symmetrySE" = "21",
                                "fractal_dimensionSE" = "22",
                                "radiusW" = "23",
                                "textureW" = "24",
                                "perimeterW" = "25",
                                "areaW" = "26",
                                "smoothnessW" = "27",
                                "compactnessW" = "28",
                                "concavityW" = "29",
                                "concave_pointsW" = "30",
                                "symmetryW" = "31",
                                "fractal_dimensionW" = "32")

#Check specs/data type of the data:
spec(Data)
```

There are no missing value in the dataset. Descriptive Statistics on numeric variables and target variable are non normal. Target variable diagnosis was plot in Figure 2. All attributes histograms were plotted in Figure 3 and Figure 4 by diagnosis. The figures show that there are differences between B and M Diagnosis such as area, compactness, perimeter, radius, etc.
```{r check, include=FALSE}
head(Data)
#tail(Data)
sapply(Data, function(x) sum(is.na(x)))
```


```{r, message=FALSE, warning=FALSE, include=FALSE, fig.cap='Table 1: Baseline attributes'}

Data_drop <- Data[,-1] #drop ID
psych::describe(Data_drop) # Describe data
#psych::describeBy(Data_drop,Data_drop$Diagnosis) #Describe data by Diagnosis

#Data_drop$Diagnosis <- as.factor(Data_drop$Diagnosis)
#summary(Data_drop$Diagnosis)
```

```{r, message=FALSE, warning=FALSE, echo=FALSE, fig.align='center', fig.cap='Figure 2ÔºöDiagnosis percentages'}
ggplot(Data,aes(x = DiagnosisÔºâ) +
    geom_bar(aes(y = ..count../sum(..count..), fill = Diagnosis)) + 
        ylab("Percent") + 
            ggtitle("Diagnosis percentages")
    
```
```{r, echo = FALSE, message = FALSE, fig.align='center', fig.cap='Figure 3: Baseline variable histogram'}
#Plot histogram for all Variables
Data_drop %>%
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) + 
    geom_histogram() + 
    facet_wrap(~key, scales = "free")
```
```{r, echo = FALSE, fig.align='center', fig.cap='Figure 4: Baseline variable by Diagnosis histogram'}
#Plot histogram for all Variables by Diagnosis
Data_drop %>%
  gather(key = "Var", value = "value", -Diagnosis) %>% 
  ggplot(aes(x=value, fill=Diagnosis)) + 
    geom_histogram(bins = 60) + 
    facet_wrap(~Var, scales = "free")
```

Correlaion analysis was done by spearman test, since most variables are non normal distributed. Dark colors are highly correlated (Figure 5).
```{r, echo=FALSE, fig.align='center', fig.cap='Figure 5: Correlation heatmap'}
#create correlation matrix
Cor <- Data_drop %>% 
       select(-Diagnosis) %>%
       cor(method="spearman")

corrplot(Cor, type="upper", order="hclust")
```
Where variables with correlation >0.9 was identified, and the variables with the largest mean absolute correlation were dropped. 18 features were left. Also, dimension Reduction was done by using PCA (Figure 6). Correlated variables such as area, perimeters, radius are grouped together and they are important contributers.\

```{r, include=FALSE}
#Removing all features with a correlation higher than 0.7, keeping the feature with the lower mean. 

highlyCor <- colnames(Data_drop[,-1])[findCorrelation(Cor, cutoff = 0.9, verbose = TRUE)]
```


```{r, include=FALSE}
highlyCor #highly correlted variables

Data_drop_cor <- Data_drop[, which(!colnames(Data_drop) %in% highlyCor)]#Drop highly correlcted variables

```

```{r, include=FALSE}
## Number of features left
ncol(Data_drop_cor)-1
```


```{r, include =FALSE}
## Data Transformation/ Dimension Reduction by using PCA, after dropping label.
#Drop Diagnosis
Data_pca <- prcomp(Data_drop[,-1], center=TRUE, scale=TRUE)
```


```{r, include=FALSE}
## PCA components summary for raw data
summary(Data_pca)#need 10 components for variance over 95%
```


```{r, include=FALSE}
## PCA without highly correlated variables
Data_pca_re <- prcomp(Data_drop_cor[,-1], center=TRUE, scale=TRUE)
summary(Data_pca_re)#7 components have variance over 97%
```


```{r, include=FALSE}
## Plot first 2 PCA components
pca_df <- as.data.frame(Data_pca$x)
ggplot(pca_df, aes(x=PC1, y=PC2, col=Data$Diagnosis)) + geom_point(alpha=0.5)
```


```{r, include=FALSE}
## Plot PCA without highly correlated variables on the first 2 components
pca_df_re <- as.data.frame(Data_pca_re$x)
ggplot(pca_df_re, aes(x=PC1, y=PC2, col=Data$Diagnosis)) + geom_point(alpha=0.5)
```


```{r, message = FALSE, include=FALSE }
## Visualize which variables are the most influential. Individuals with a similar profile are grouped together
library(factoextra)
#fviz_eig(Data_pca)
fviz_pca_var(Data_pca,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             )
```


```{r, echo=FALSE, fig.align='center', fig.cap='Figure 6: PCA first two components, Individuals with a similar profile are grouped together'}
## After correlated variables were dropped
#fviz_eig(Data_pca_re)
library(factoextra)
fviz_pca_var(Data_pca_re,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             )

```


## Model Training Result
First two models are evaluated by using library(e1071), data is not scaled with default parameters Cost =1 and gamma = 1/(data dimension). Original data with or without variable selection were used. Two models have low accuracy under 0.8.\
Additional five models were evaluated by using caret package to fit models with default parameter, tuning  with 5 fold cross validation, repeated 5 times; Caret "svmRadial" model uses kernlab package. $C = 2^{((1:len) - 3)}$ (C=0.25) and sigma is defined by Kernlab package's sigest function, sigest estimates are based upon the 0.1 and 0.9 quantile of $||x-x'||^2$. Basically any value in between those two bounds will produce good results. A vector of length 3 defining the range (0.1 quantile, median and 0.9 quantile) of the sigma hyperparameter. \
Training models are trained based on original data, data with variable selection, original data scaled, data sacled and PCA, data with variable selection scaled and PCA. For comparison reason, anohter two models were trained with neural network, with or without pca for each. Training model performance in terms of accuracy is in Figure 7 for the seven models. Test model performance in terms of accuracy is in Figure 8.

```{r, include=FALSE}
## Split data into training and test
set.seed(1234) # so that the indices will be the same when re-run
trainIndices = createDataPartition(Data_drop$Diagnosis, p=.8, list=F)

train_raw = Data_drop %>% 
  slice(trainIndices)

nrow(train_raw)

test_raw = Data_drop %>% 
  slice(-trainIndices)
head(test_raw)

#verify if the randomization process is correct
prop.table(table(train_raw$Diagnosis))
prop.table(table(test_raw$Diagnosis))
```


```{r, include=FALSE}
## split data into training and test after dropping correlated variables
#for multicolinearty droped Data_drop_cor
set.seed(1234) # so that the indices will be the same when re-run
trainIndices_drop = createDataPartition(Data_drop_cor$Diagnosis, p=.8, list=F)

train_drop_raw = Data_drop_cor %>% 
  slice(trainIndices_drop)

nrow(train_drop_raw)
head(train_drop_raw)

test_drop_raw = Data_drop_cor %>% 
  slice(-trainIndices_drop)
head(test_drop_raw)

#verify if the randomization process is correct
prop.table(table(train_drop_raw$Diagnosis))
prop.table(table(test_drop_raw$Diagnosis))
```


```{r svm raw 1, include=FALSE}
## Svm with original data by Using library(e1071), data is not scaled; 
## * Default parameters: Cost =1, gamma = 1/(data dimension)
## first gamma:
svm_raw <- svm(Diagnosis ~ ., data=train_raw, scale = FALSE, kernel =
"radial", type = "C-classification")
#svm_pca <- svm(Diagnosis ~ ., data=Data_pca)
svm_raw$gamma

pred_svm <- predict(svm_raw, test_raw)

cm_svm <- confusionMatrix(pred_svm, as.factor(test_raw$Diagnosis), positive = "M")
cm_svm
#summary(svm_raw)
```


```{r svm_drop_raw 2, include=FALSE}
## Svm with after droped correlated variables with default setting by using library(e1071)
## second gamma:
svm_drop_raw <- svm(Diagnosis ~ ., data=train_drop_raw, scale = FALSE, kernel =
"radial", type = "C-classification")
#svm_pca <- svm(Diagnosis ~ ., data=Data_pca)
svm_drop_raw$gamma

pred_drop_svm <- predict(svm_drop_raw, test_drop_raw)

cm_drop_svm <- confusionMatrix(pred_drop_svm, as.factor(test_drop_raw$Diagnosis), positive = "M")
cm_drop_svm
```


```{r svm caret raw 1, include=FALSE}
## Use caret package to fit models with default parameter tuning  with 5 fold cross validation, repeated 5 times; Caret "svmRadial" model uses kernlab package; No data scaling.

### * $C = 2^{((1:len) - 3)}$ (C=0.25) 
### * Sigma is defined by Kernlab package's sigest function, sigest estimates are based upon the 0.1 and 0.9 quantile of $||x-x'||^2$. Basically any value in between those two bounds will produce good results. A vector of length 3 defining the range (0.1 quantile, median and 0.9 quantile) of the sigma hyperparameter.
#fitControl <- trainControl(method = "none", classProbs = TRUE)
fitControl <- trainControl(method="repeatedcv",
                            number = 5,
                           repeats =5,
                            preProcOptions = list(thresh = 0.8), # threshold for pca preprocess
                            classProbs = TRUE,
                            summaryFunction = twoClassSummary)
set.seed(825)
svm_raw_caret <- train(Diagnosis~.,
                    data = train_raw,
                    method="svmRadial",
                    metric="ROC",
                    trace=FALSE,
                    trControl=fitControl)
svm_raw_caret$finalModel
pred_svm_caret <- predict(svm_raw_caret, test_raw)
cm_svm_caret <- confusionMatrix(pred_svm_caret, as.factor(test_raw$Diagnosis), positive = "M")
cm_svm_caret

```

```{r svm caret drop raw 2, include=FALSE}
## Use caret package to fit models after droped correlated variables with default parameter tuning with 5 fold cross validation, repeated 5 times; No data scaling.

### **It's not as good as the previous model with all features but it's less sensitive on negative predicting.**
#fitControl <- trainControl(method = "none", classProbs = TRUE)
fitControl <- trainControl(method="repeatedcv",
                            number = 5,
                           repeats =5,
                            preProcOptions = list(thresh = 0.8), # threshold for pca preprocess
                            classProbs = TRUE,
                            summaryFunction = twoClassSummary)
set.seed(825)
svm_drop_caret <- train(Diagnosis~.,
                    data = train_drop_raw,
                    method="svmRadial",
                    metric="ROC",
                    trace=FALSE,
                    trControl=fitControl)
svm_drop_caret$finalModel
pred_drop_svm_caret <- predict(svm_drop_caret, test_drop_raw)
cm_drop_svm_caret <- confusionMatrix(pred_drop_svm_caret, as.factor(test_drop_raw$Diagnosis), positive = "M")
cm_drop_svm_caret
```


```{r svm scale 3, include=FALSE}
# **Scale variables and cross validation**
fitControl <- trainControl(method="repeatedcv",
                            number = 5,
                           repeats =5,
                            preProcOptions = list(thresh = 0.8), # threshold for pca preprocess
                            classProbs = TRUE,
                            summaryFunction = twoClassSummary)
set.seed(825)
svm_scale <- train(Diagnosis~.,
                    data = train_raw,
                    method="svmRadial",
                    metric="ROC",
                    preProcess=c('center', 'scale'),
                    trace=FALSE,
                    trControl=fitControl)
svm_scale$finalModel
pred_svm_scale <- predict(svm_scale, test_raw)
cm_svm_scale <- confusionMatrix(pred_svm_scale, as.factor(test_raw$Diagnosis), positive = "M")
cm_svm_scale

```


```{r svm scale pca 4, include=FALSE}
## Scale variables with pca thresdhold = 0.8. 0.8 is the sweet spot.
fitControl <- trainControl(method="repeatedcv",
                            number = 5,
                           repeats =5,
                            preProcOptions = list(thresh = 0.8), # threshold for pca preprocess
                            classProbs = TRUE,
                            summaryFunction = twoClassSummary)
set.seed(825)
svm_scale_pca <- train(Diagnosis~.,
                    data = train_raw,
                    method="svmRadial",
                    metric="ROC",
                    preProcess=c('center', 'scale','pca'),
                    trace=FALSE,
                    trControl=fitControl)
svm_scale_pca$finalModel
pred_svm_scale_pca <- predict(svm_scale_pca, test_raw)
cm_svm_scale_pca <- confusionMatrix(pred_svm_scale_pca, as.factor(test_raw$Diagnosis), positive = "M")
cm_svm_scale_pca

```

```{r svm pca 5, include=FALSE}

## Scale variables with dropped variables and with pca = 0.95; 

### **The results doesn't improve much anymore, with increasing threshold of pca, the performance increases on the 4th digit and it's more stable**
fitControl <- trainControl(method="repeatedcv",
                           number = 5,
                            repeats =5,
                            preProcOptions = list(thresh = 0.95), # threshold for pca preprocess
                            classProbs = TRUE,
                            summaryFunction = twoClassSummary)
set.seed(825)
svm_drop_pca  <- train(Diagnosis~.,
                    data = train_drop_raw,
                    method="svmRadial",
                    metric="ROC",
                    preProcess=c('center', 'scale','pca'),
                    trace=FALSE,
                    trControl=fitControl
                    #tuneLength = 8
                    )
svm_drop_pca
pred_svmdrop_pca <- predict(svm_drop_pca, train_drop_raw)
cm_svmdrop_pca <- confusionMatrix(pred_svmdrop_pca, as.factor(train_drop_raw$Diagnosis), positive = "M")
cm_svmdrop_pca

```


```{r svm grid, include=FALSE }
# Tune SVM parameters by grid search, total 42 pairs
svmGrid <-  expand.grid(sigma = c(.01, .015, 0.2,0.25, 0.275,0.3),
                    C = c(0.25, 0.5, 0.75, 0.9, 1, 1.1, 1.25))
                    
nrow(svmGrid)
```


```{r svm tune, include=FALSE }
fitControl <- trainControl(method="repeatedcv",
                           number = 5,
                            repeats =5,
                            preProcOptions = list(thresh = 0.8), # threshold for pca preprocess
                            classProbs = TRUE,
                            summaryFunction = twoClassSummary)
set.seed(825)
svm_tune <- train(Diagnosis ~ ., 
                 data = train_drop_raw, 
                 method="svmRadial",
                 metric="ROC",
                 preProcess=c('center', 'scale','pca'),
                 trace=FALSE,
                 trControl=fitControlÔºå
                 tuneGrid = svmGrid)
#svm_tune
#ten fold The final values used for the model were sigma = 0.015 and C = 0.9.
#five fold The final values used for the model were sigma = 0.015 and C = 0.75.
#The final values used for the model were sigma = 0.015 and C = 1.

```


```{r svm tune plot, include=FALSE}
## Plot the results on the feature selected data with pca, by grid search 
plot(svm_tune)  
```


```{r svm tune predict, include=FALSE}
## Performance on test data with the best parameters
pred_svm_tune <- predict(svm_tune, train_drop_raw)
cm_svm_tune <- confusionMatrix(pred_svm_tune, as.factor(train_drop_raw$Diagnosis), positive = "M")
cm_svm_tune

#Plotting the Resampling Profile: Examine the relationship between the estimates of performance and the tuning parameters

```


```{r neural net scale, include=FALSE}
## Neural network
fitControl <- trainControl(method="repeatedcv",
                           number = 5,
                            repeats =5,
                            classProbs = TRUE,
                            summaryFunction = twoClassSummary)
set.seed(825)
model_nnet <- train(Diagnosis~.,
                    data = train_drop_raw,
                    method="nnet",
                    metric="ROC",
                    preProcess=c('center', 'scale'),
                    trace=FALSE,
                    tuneLength=10,
                    trControl=fitControl)
#model_nnet
pred_nnet <- predict(model_nnet, test_drop_raw)
cm_nnet <- confusionMatrix(pred_nnet, as.factor(test_drop_raw$Diagnosis), positive = "M")

# #grid search for nnet
# nnetGrid <-  expand.grid(size = seq(from = 1, to = 10, by = 1),
#                         decay = seq(from = 0.1, to = 0.5, by = 0.1))
```
```{r, include=FALSE}
cm_nnet
```


```{r neural net scale pca, include=FALSE}
## Neural network with pca
set.seed(825)
fitControl <- trainControl(method="repeatedcv",
                           number = 5,
                            repeats =5,
                            preProcOptions = list(thresh = 0.8), # threshold for pca preprocess
                            classProbs = TRUE,
                            summaryFunction = twoClassSummary)
nnet_pca <- train(Diagnosis~.,
                 data = train_raw,
                 method = "nnet",
                 metric = "ROC",
                 preProcess=c('center', 'scale','pca'),
                 trace=FALSE,
                 tuneLength=10,
                 trControl = fitControl)
#nnet_pca
pred_nnet_pca <- predict(nnet_pca, test_raw)
cm_nnet_pca <- confusionMatrix(pred_nnet_pca, as.factor(test_raw$Diagnosis), positive = "M")
cm_nnet_pca 

```
```{r, include=FALSE}
cm_nnet_pca 
```


```{r, echo=FALSE, fig.align='center', fig.cap='Figure 7: Model Accuracy on training data' }
# Models evaluation by caret package with cross validation on training model
model_list <- list(Raw_svm = svm_raw_caret, Select_svm = svm_drop_caret, 
                   Svm_scale= svm_scale, Svm_scale_pca=svm_scale_pca, 
                   Select_svm_pca=svm_drop_pca, Nnet=model_nnet,
                   Nnet_pca=nnet_pca)
resamples <- resamples(model_list)
bwplot(resamples, metric = "ROC")
```



```{r, echo=FALSE, fig.align='center', fig.cap='Figure 8: Model Accuracy on test data'}
# Overall model evaluation on test data
## **Final model achieved accuracy of 0.9825.**
overall <- data.frame(model = rep(c("Raw_svm", "Select_svm","Svm_scale", "Svm_scale_pca", "Select_svm_pca", "Nnet", "Nnet_pca")),
                      rbind(cm_svm$overall,
                            cm_drop_svm$overall,
                            cm_svm_scale$overall,
                            cm_svm_scale_pca$overall,
                            cm_svmdrop_pca$overall,
                            cm_nnet$overall,
                            cm_nnet_pca$overall))
#overall
overall_gather <- overall[,1:3] %>%
  gather(measure, value, Accuracy:Kappa)
#overall_gather
byClass <- data.frame(model = rep(c("Raw_svm", "Select_svm","Svm_scale", "Svm_scale_pca", "Select_svm_pca", "Nnet", "Nnet_pca")),
                      rbind(cm_svm$byClass,
                            cm_drop_svm$byClass,
                            cm_svm_scale$byClass,
                            cm_svm_scale_pca$byClass,
                            cm_svmdrop_pca$byClass,
                            cm_nnet$byClass,
                            cm_nnet_pca$byClass)
                      )
#byClass  
byClass_gather <- byClass[,1:3] %>%
  gather(measure, value, Sensitivity:Specificity)
#byClass_gather

overall_byClass_gather <- rbind(overall_gather, byClass_gather)
overall_byClass_gather <- within(overall_byClass_gather, model <- factor(model, levels = c("Raw_svm", "Select_svm","Svm_scale", "Svm_scale_pca", "Select_svm_pca", "Nnet", "Nnet_pca")))

#overall_byClass_gather
OV<-overall_byClass_gather %>% 
  filter(measure == "Accuracy")  %>% 
  arrange(value)

#OV

Per<-ggplot(OV, aes(x = model, y = value)) +
    geom_point(shape=1)  +
    scale_x_discrete(labels = abbreviate) +
    scale_y_continuous(breaks=c(0.6,0.8, 0.94, 0.96,0.97,0.98, 0.99)) +
    labs(caption = "Order of models from the left to right: 
         Raw_svmÔºöSVM with original data, 
         Select_svm: SVM with variable selection,
         Svm_scale: SVM with original data scaled, 
         Svm_scale_pca: SVM with data sacled and PCA, 
         Select_svm_pca: SVM with variable selection, scaled and PCA, 
         Nnet: Nueral network, 
         Nnet_pca: Nueral network with PCA")

Per
```



# Conclusion
SVM with RBF could achieve the model accuracy of 0.9825 on the test data. With variable selection and demension deduction by using PCA, SVM has stable and good performance. 

# Discussion
There are a few key points to consider during the analysis process listed as below: 

1. Scaling variables is very important. and tuning parameters by using resampling technique for example, cross validation is more important.

2. Reduce multicolinearity could stablize the model performance.

3. Dimention deduction (using PCA here) with out variable selection is not robust. It would takes time to find a sweet spot on the optimal PCA value.

4. In another word, with the appropriate variable selection, it reduces the variation of dimention deduction with a stable result. 

5. The best tuning parameters on training could be overfitting. Neural network tends to overfit the model.

6. Grid search method could be improved by combing performance on test data instead of sololy relys on training data. Further research could be done on find the best parameters to improve performance on test data.\
&nbsp;


## Reference
1) Robert Hable. (2011) "Asymptotic normality of support vector machine variants and other regularized kernel methods"
2) Bai et al. (2019) "Inference for Support Vector Machine under L1 Regularization"
3) Wang et al. (2019) "Distributed Inference for Linear Support Vector Machine"\
&nbsp;
&nbsp;

### Appendix
```{r ref.label=knitr::all_labels(), echo = T, eval = F}

```




